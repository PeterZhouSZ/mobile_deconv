% Use the IITM Dissertion class
\documentclass[BTech]{iitmdiss}
\usepackage{times}
\usepackage{t1enc}
\usepackage{float}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage[hypertex]{hyperref} % hyperlinks for references.
\usepackage{amsmath} % easier math formulae, align, subequations \ldots

% Enable code snippets coloring
\usepackage{listings}
\usepackage{xcolor}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

% Python details
\lstdefinestyle{pylisting}{frame=none,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true
  tabsize=2
}

% C# details
\lstdefinestyle{sharpclisting}{frame=none,
  language=[Sharp]C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true
  tabsize=2
}

\lstset{language=Python, frame=none}
\lstset{language=[Sharp]C, frame=none}

\begin{document}
%----------- Title page ------------------------------------------------
\title{Analyzing motion sensors on mobile platform for image restoration}
\author{Saragadam R V Vishwanath (EE10B035)}
\date{MAY 2014}
\department{ELECTRICAL ENGINEERING}
\maketitle
\pagebreak

%----------- Certificate -----------------------------------------------
\certificate

\vspace*{0.5in}

\noindent This is to certify that the thesis titled {\bf Analyzing
 motion sensors on mobile platform for image restoration}, submitted by
  {\bf Saragadam Raja Venkata Vishwanath}, 
  to the Indian Institute of Technology, Madras, for
the award of the degree of {\bf Bachelor of Technology}, is a bona fide
record of the research work done by him under our supervision.  The
contents of this thesis, in full or in parts, have not been submitted
to any other Institute or University for the award of any degree or
diploma.

\vspace*{1.5in}

\begin{singlespacing}
\hspace*{-0.25in}
\parbox{2.5in}{
\noindent {\bf Prof. A. N. Rajagopalan} \\
\noindent Research Guide \\ 
\noindent Professor \\
\noindent Dept. of Electrical Engineering\\
\noindent IIT-Madras, 600 036 \\
} 
\hspace*{1.0in} 
\end{singlespacing}
\vspace*{0.25in}
\noindent Place: Chennai\\
Date: 12th May 2014

\pagebreak

%----------- Acknowledgement -------------------------------------------
\acknowledgements

I would like to express my heartfelt gratitude to my guide, 
Dr. A. N. Rajagopalan for his invaluable support and guidance throughout
my project duration. I have gained immense insight into computational
photography and image processing in this one year, which I believe will 
hold me in good stead in my future endeavors. I am indebted to him for
patiently sitting with me at times when I could not see any path forward
. 

\noindent I also thank all my lab mates for being extremely helpful and
 for their patience and guiding me through difficult tasks. I had the
  privilege of working with some of the best minds in the department.

\noindent I would also like to thank my parents and my brother for 
giving me feedback on my project work by evaluating my mobile
 application.

% Need to write my name on the right side.
\raggedright{Vishwanath}

%----------- Abstract --------------------------------------------------
\abstract
We explore various image restoration and registration techniques using 
data obtained from a mobile device. To evaluate the usability of the 
mobile platform, we evaluate image deconvolution, image registration,
depth estimation and image super resolution algorithms with the obtained
image data. We compare the results to the state of the art algorithms
to get a feel for the superiority of the mobile platform. Further, the
report also gives an indepth discussion about the future of the project
on computational photography on the mobile platform.

%----------- Table of content ------------------------------------------
\begin{singlespace}
\tableofcontents
\thispagestyle{empty}

\listoftables
\addcontentsline{toc}{chapter}{LIST OF TABLES}
\listoffigures
\addcontentsline{toc}{chapter}{LIST OF FIGURES}
\end{singlespace}
\pagebreak

% We might not need abbrevations and notations.
%----------- Abbrevations ----------------------------------------------
\abbreviations

\noindent 
\begin{tabbing}
xxxxxxxxxxx \= xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \kill
\textbf{IITM}   \> Indian Institute of Technology, Madras \\
\textbf{WP8}    \> Windows Phone 8 \\
\textbf{FFT}    \> Fast Fourier Transform \\
\textbf{DFT}    \> Discrete Fourier Transform \\
\textbf{SSIM}   \> Structural Similarity Measure \\
\end{tabbing}

\pagebreak

%----------- Notations -------------------------------------------------
\chapter*{\centerline{NOTATION}}
\addcontentsline{toc}{chapter}{NOTATION}

\begin{singlespace}
\begin{tabbing}
xxxxxxxxxxx \= xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \kill
\textbf{$im(x,y)$}  \> Image intensity at location (x, y) \\
\textbf{$\delta(x,y)$}  \> The discrete time 2D dirac delta function \\
\textbf{$im{\Leftrightarrow}IM$} \> $IM$ and $im$ are the fourier transform 
pairs, with $im$ being the spacial domain representation and $IM$ being
the frequency domain representation.
\end{tabbing}
\end{singlespace}

\pagebreak

% Start numbering the pages from now.
\pagenumbering{arabic}

%----------- Introduction ----------------------------------------------
\chapter{INTRODUCTION}
\label{chap:intro}
There has been a recent shift in computation from traditional computers
to ubiquitous mobiles. With the ever increasing power of the mobiles, 
executing many of the image processing algorithms on mobile has been 
possible. Further, the many peripherals available for the mobile, like
the inertial sensors, controllable focus, TCP, bluetooth etc., 
computational photography is greatly advanced. 

Using the mobile platform for computational photography has the unique
advantages of capturing motion during exposure time, which can be used
for estimating the blur kernel or estimating the direction of motion
for reducing the search space of image registration. Further, 
controllable focus and the ability to capture fast multiple images means
that it is easy to implement multichannel super resolution or deblurring
algorithms. 

To understand the potential of a mobile for computational photography, 
we choose to evaluate various algorithms on the mobile and evaluate the 
performance. For this purpose, we have chosen the Nokia Lumia 520 mobile
which features Windows Phone 8 operating system. 

The project report consists of the following part. The chapter on 
\textbf{THEORY} presents a brief background of the various algorithms
and the mathematics involved in the project. \textbf{DEVICE} gives an
in-depth account of the mobile platform, the application written for the
mobile platform and the host side software. \textbf{DEBLURRING} 
explains the implementation of the deblurring algorithm using the data
obtained from the mobile. \textbf{DEPTH ESTIMATION} has two parts, 
\emph{Depth estimation using motion blur} and \emph{Depth estimation 
using focus measure}. \textbf{IMAGE REGISTRATION} discusses about how
inertial sensor data can be used to reduce the search space of image 
registration process. Finally, \textbf{SUPER RESOLUTION} looks at how 
multiple frames can be used to super resolve an image.

The end of the project report has in-depth discussion regarding future
usage of the mobile application as a general prototyping platform and a
brief discussion of the \emph{github} version control system.

%----------- Basic Theory ----------------------------------------------
\chapter{BASIC THEORY}
\label{chap:basic_theory}
In this chapter, we look briefly at the back ground which is necessary 
to understand the forthecoming chapters. 

\section{Position from acceleration data}
\label{basic_theory:accel}
A major part of the project involves estimating the trajectory of motion
from accelerometer data. In the current context, we assume that we have
access to a 3-axis accelerometer alone. The 3-axis accelerometer 
measures physical acceleration, which is a combination of the 
acceleration due to gravity and any external force. Further, the 
measured values are relative to the mobiel frame of reference. Hence, 
we can model our measured acceleration as
\begin{align*}
\hat{a}_x= g_x + a_x + n_x\\
\hat{a}_y= g_y + a_y + n_y\\
\hat{a}_z= g_z + a_z + n_z
\end{align*}
Where $n_x, n_y$ and $n_z$ are noise in measurements.For calculating
 the trajectory, we need $\{a_x,a_y,a_z\}$. To obtain 
these values, we need to subtract the acceleration due to gravity. 
However, we have no further information apart from the above equations.
To get a good estimate of the required acceleration, we assume that the
gravity vector is quasi-static when there is only translation in the 
mobile device. Hence, we estimate our gravity vector as follows
\begin{align*}
g_x= \alpha{g_x} + (1-\alpha)\hat{a}_x\\
g_y= \alpha{g_y} + (1-\alpha)\hat{a}_y\\
g_z= \alpha{g_z} + (1-\alpha)\hat{a}_z
\end{align*}
Once we have the device acceleration, the equation for finding the 
trajectory of motion would be
\begin{align*}
x(t)=\int_0^{t}\int_0^{t}{a_x(t)dt}\\
y(t)=\int_0^{t}\int_0^{t}{a_y(t)dt}\\
z(t)=\int_0^{t}\int_0^{t}{a_z(t)dt}
\end{align*}
The discrete time approximation is
\begin{align*}
x[n]=({\sum}_{t}^{n}{\sum}_{t}^{n}a_x[n]){\times}T^2\\
y[n]=({\sum}_{t}^{n}{\sum}_{t}^{n}a_y[n]){\times}T^2\\
z[n]=({\sum}_{t}^{n}{\sum}_{t}^{n}a_z[n]){\times}T^2
\end{align*}
Though this gives a good estimate of the orientation of the device when
there is no motion or the device acceleration when there is only
translation, the method suffers from two drawbacks
\begin{enumerate}
\item The noise in measurement creates a random walk pattern. This might
not be of major concern as it would only create a small disturbance in 
the over all trajectory.
\item If the estimate of gravity vector is not correct, it would create
a DC offset to the device acceleration. This gives rise to drift, which
blows up as $t^2$ which can give rise to wrong trajectory.
\end{enumerate}
The following graph explains how the drift can be of serious concern when
estimating the trajectory. 

\begin{figure}[ht]
\begin{center}
\resizebox{150mm}{!} {\includegraphics *{images/drift_image.png}}
\caption {Plot showing the effect of DC offset to acceleration value.
The drift was iterated from -1\% to 1\% of the maximum acceleration 
value.}
\label{fig:drift_example}
\end{center}
\end{figure}

Due to lack of information regarding the orientation of the mobile, which
gives rise to a bad estimate of gravity vector, the drift will exist 
and make the estimate unreliable. In the forthecoming chapters, we will
see the consequences of have a bad trajectory and possible ways to 
rectify the problem.

\section{Image registration}
\label{basic_theory:registration}
Image registration finds application in many areas like medical imaging,
aerial photography and automated target recognition. In computational
photography, image registration is used in a wide areas like panorama
stitching, depth estimation using stereo vision and video stabilization.

Image registration involves finding the transform between a pair of 
images. The transform could be a simple translation and rotation or 
complicated warping. For simple cases like translation only model or 
rotation only model, many of the frequency based methods give good 
results. For advanced models, feature based registration is used. The 
SIFT based image registration, for example, identifies key points in
each image. Once we have the corresponding points in both the images,
the problem is of estimating the matrix that created this transform.

Note that when there is not much information available or we have a 
blurred and latent image pair, registration becomes difficult. Section
\ref{chap:image_registration} has discussion about how this can be 
simplified using data from accelerometer. In this section, we will look
at a simple case of registering image pair which are transformed by shift
alone. % And rotation and scale? It would be good because then we can
% compare it in the image registration section.

\subsection{Image registration using cross correlation}
Let $im_1$ and $im_2$ be the image pairs. Since we are looking at a 
translation only model, we have
\begin{align*}
im_2(x,y)=im_1(x-x_0, y-y_0)
\end{align*}
Given these two images, we wish to estimate $(x_0,y_0)$. A shift in the
spacial domain reflects as a phase multiplication in the frequency domain.
Hence, if $IM_1$ is the DFT of $im_1$, then,
\begin{align*}
IM_2(u,v) = IM_1(u,v)e^{-j(\frac{2{\pi}x_0u}{M}+\frac{2{\pi}y_0u}{N})}
\end{align*}
Let
\begin{align*}
H(u,v) = \frac{IM_1(u,v){\times}IM_2^*(u,v)}{|IM_1(u,v){\times}IM2(u,v)|}\\
\implies H(u,v) = e^{-j(\frac{2{\pi}x_0u}{M}+\frac{2{\pi}y_0u}{N})}
\end{align*}
This gives,
\begin{align*}
h(x,y) = \delta(x_0, y_0)
\end{align*}
Hence, the location of peak in h(x,y) will give the shift between the 
two images. 
\section{Image blur}
\label{basic_theory:image_blur}
Image blur is equivalent to low pass filtering in 1D signals. While 
image blur is desired in some applications like animation, it is 
undesirable when taking still photos. Two types of blurs are of interest
in this project, namely defocus blur and motion blur

\subsection{Defocus blur}
\label{basic_theory:image_blur:optical}
Defocus blur is caused when the image plane does not match with the 
optical plane of the object. This arises only in real aperture cases
when the focal length of the lens is finite and objects are at varied
distances. 

The blur that arises due to defocus can be modeled as a 2D gaussian
filtering operation. Hence, the point spread function can be modeled as
\begin{align*}
h(x,y) = \frac{1}{2\pi\sigma^2}e^\frac{-(x^2+y^2)}{2\sigma^2}
\end{align*}
Where $\sigma$ depends on the depth of the object from the lens. For a 
given lens to sensor distance, $\sigma$ increases with increasing depth.

The defocus blur can be used in the shape from focus method, which 
varies one particular parameter of the system like the lens position or
the object position to bring the object in focus. This variation gives 
an estimate of the depth. Further discussion about shape from focus will
be carried out in the chapter on Depth Estimation.

\subsection{Motion blur}
\label{basic_theory:image_blur:motion}
Motion blur occurs due to relative motion between the scene and the 
camera during the exposure time. This could mean the object in the scene
is moving or there is a camera shake. This report deals with the problem
of camera shake.

The process of camera shake can be considered as an averaging of 
multiple sharp images shifted due to the camera motion. Hence for a 
planar scene, 
\begin{align*}
\hat{im}(x,y) = \sum_tim(x-x_t,y-y_t)\\
\implies\hat{im}(x,y) = \sum_t\delta(x_t, y_t)*im(x,y)
\end{align*}
where $\delta(x_t, y_t)$ is an impulse at $(x_t, y_t)$ and $\hat{im}$ is
the observed image.

This can be rewritten as
\begin{align*}
\hat{im}(x,y) = (\sum_t\delta(x_t, y_t))*im(x,y)\\
\implies \hat{im}(x,y) = h(x,y)*im(x,y)
\end{align*}

$h(x,y)$ is called the point spread function. If the image is not a 
planar scene, then every point in the scene moves by a different amount
due to the camera shake. If we assume that there is no parallax error,
we have,
% Do we need to write about why they move by different lengths?
\begin{align*}
\hat{im}(x,y) = \sum_t\delta(x_t.k(x,y), y_t.k(x,y))*im(x,y)
\end{align*}
where $k(x,y)$ is the depth of the image at $(x,y)$. This space variant
blurring operation can be used to calculate depth information from the 
images, as objects at different depths will be blurred to a different
extent. Calculating $k(x,y)$ gives the depth map of the image

\section{Image deconvolution}
\label{basic_theory:deconv}
The process of removing the blur in the image is known as image
deconvolution. Deconvolution can be broadly of two types, blind and non-
blind. If the psf is known aprior, it is known as non-blind 
deconvolution and if it not know, it is called blind deconvolution. 
Blind deconvolution is an ill-posed problem, as any combination of 
latent image and psf can give rise to the observed image. Hence, various
constraints are used when trying to retrieve the latent image. For 
example, an image can be considered as piecewise smooth function with
sharp images. Hence, we can try to reduce the total variance of the 
obtained latent image. % Should we write about TV stuff?

On the other hand, non-blind deconvolution is less ill posed than the
blind case, as we have the psf estimate. As discussed in the previous 
section, the blurred image can be represented as the convolution of 
the psf and the latent image in case of a planar scene. This 
transforms into multiplication in the frequency domain. Hence, we can
simply divide the image's fourier transform with the psf's fourier
transform to get the original image. i.e,
\begin{align*}
IM_{latent} = \frac{\hat{IM}}{PSF}
\end{align*}
Where $IM$ is the discreet fourier transform of $im$. However, the 
observed image is further degraded by noise. Hence, the actual model of
the observed image is
\begin{align*}
\hat{im}(x,y) = h(x,y)*im(x,y) + n(x,y) 
\end{align*}
Where $n(x,y)$ is the noise. Hence, the equation in the frequency domain
can be loosely expressed as
\begin{align*}
\hat{IM} = H.IM + N\\
IM_{latent} = \frac{\hat{IM}}{H}\\
\implies IM_{latent} = IM + \frac{N}{H}
\end{align*}
Note that $h(x,y)$ has a low frequency structure and noise has a high 
frequency structure. Hence, the division will blow up the calculated 
image. 

\subsection*{Wiener deconvolution}
\label{basic_theory:deconv:wiener}
Instead of simple division, we reformulate our problem as finding a 
filter which when convolved with the observed image would return the
latent image. Further, we wish to reduce the error between the latent
image and the image obtained by filtering the observed image, i.e
\begin{align*}
minimize \hspace*{4mm} E|im(x,y)-f(x,y)*\hat{im}(x,y)|^2
\end{align*}
Where $f(x,y)$ is the desired filter and $E|.|$ is the expectation
operator. Changing to frequency domain, we
have
\begin{align*}
minimize \hspace*{4mm} E|IM-F.\hat{IM}|^2\\
\hat{IM}=IM.H + N
\end{align*}
Differentiating with respect to $F$, we get
\begin{align*}
F = \frac{H^*}{|H|^2 + \alpha}\\
\implies IM_{latent} = \hat{IM}.F\\
=\frac{H^*.\hat{IM}}{|H|^2 + \alpha}
\end{align*}
Where $\alpha$ is the inverse signal to noise ratio of the image.
% Very vague. Is that ok?

\subsection{Regularized deconvolution}
\label{basic_theory:deconv:reg}
% bleh, I have no clue what to write
Since we know that the image blows up at the edges, we can impose a 
penality on the edge weights. Let $g_x$ and $g_y$ be the 
horizontal and vertical differentiation filters . Then, we need
\begin{align*}
minimize \hspace*{4mm} |y-h*x|^2 + \alpha|g_x*x|^2 + \alpha|g_x*x|^2\\
\implies minimize \hspace*{4mm} |Y-HX|^2 + \alpha|G_xX|^2 + \alpha|G_xX|^2\\
\end{align*}
Where $X$ represents the DFT of $x$. Differentiating the above equation
with respect to $X$, we get
\begin{align*}
X=\frac{H^*Y}{|H|^2+\alpha(G_x^2+G_y^2)}
\end{align*}

This is called regularized deconvolution. 

\pagebreak
%----------- Device ----------------------------------------------------
\chapter{DEVICE}
\label{chap:device}
Primarily, we want the device to function as an acquisition system which
would take pictures and measure some sensor values. While choosing the
mobile platform, we wanted to look at the following
features
\begin{itemize}
\item The device should be low cost. We are looking at a mobile which
would not cost above Rs. 15,000. This gave us the option to choose 
Android mobiles or the low end Windows Phone Mobiles.
\item The device should have a good Software Development Kit. Android
and Windows Phone both have a very good SDK.
\item The device should have accelerometer, gyroscope and manual focus
ability. Almost all the mobiles which come today have these features 
and hence was not very critical.
\item The device should have easy image handling capabilities. Here, 
Windows Phone was a clear winner due to the powerful Nokia Imaging SDK.
\item The device should have TCP/Bluetooth capability. Again, every 
smart phone available today has these features. 
\end{itemize}
 
With the above points in mind and considering the low cost, we chose to
go for Nokia Lumia 520 (henceforth called 520) which features a 5 MP
camera, 3-axis accelerometer, WiFi and Blutooth support. However, the 
tradeoff was that we could not get a gyroscope on the mobile. This 
would be a serious hinderance for handling rotation of the mobile along
with translation. However, since we were only evaluating the 
possibilities of computational photography on the mobile platform, we
chose to ignore it and see what can be done with the accelerometer 
alone. 

From the ease of usage perspective, the Windows Phone 8 SDK has a very
simple interface with coding in C\# and GUI design using XAML script. 
Visual Studio makes it very easy to create applications (apps) fast.

% Device side application
\section{Device side application}
\label{device:device_app}
The app is mainly used for logging information to the computer so that
algorithms can be tested offline. The major advantage of this method is
that it gives the power of the computer along with the flexibility to 
experiment with various methods. 
 
To make the application versatile and useful for future development, we
added multiple features to capture images. A screenshot of the 
application is given below.
\begin{figure}[htpb]
    \begin{center}
        \resizebox{100mm}{!} {\includegraphics *{images/app_screenshot.png}}
        \caption {A screenshot of the application}
        \label{fig:app_screenshot}
    \end{center}
\end{figure}
The top right, \textbf{Information block} serves as a debugging block 
during the image capturing process. For example, when taking images with
long exposure, it indicates the number of samples of accelerometer data
obtained. The \textbf{Debug section} box indicates the focus position and other
debug information. The focus position can be controlled by the yellow
slide bar which is situated at the bottom of the application. 

The \textbf{Accelerometer} box indicates the
current value of the accelerometer. The button titled \textbf{Click} is 
used for capturing a single long exposure image. \textbf{Connect} button
is used for connecting to the computer using the TCP protocol. The Host
and the port can be selected in the fields at the right bottom of the 
application screen. Apart from these features, the following features 
add versatility to the application

\begin{itemize}
    \item \textbf{Log images} enables continuous logging of the image 
    frames to the computer when TCP stream is open. With the default
    setting, the duration between two frames is close to 20ms.
    \item \textbf{Enable delay} adds a 500 ms delay between the image
    retrieved from the image buffer and the long exposure shot.
    \item \textbf{Enable preview} captures an image from the image buffer
    (which can act as a latent image) prior to a long shot image. 
    \item \textbf{Start sensor log} enables continous logging of the 
    accelerometer data to the computer when the TCP stream is open. 
    \item \textbf{Focus sweep} records 100 images with varied focus 
    distance and sends them to the computer.
\end{itemize}

The application has a 10ms timer, which takes care of all the logging 
and image capture functions. While it is more apt to use a separate 
thread, we found that using a timer was an easy method. 

We discuss some of the methods implemented for logging the data and 
sending it over TCP to understand the intricacies involved.

% Logging accelerometer data during exposure
\subsection{Logging accelerometer data during exposure}
\label{device:device_app:cam}
When the \textbf{Click} button is toggled, a camera capture sequence
is started, which is programmed to take a picture with an exposure time
of 200ms (adjustable). However, by default, this suspends all other 
thread activities and is only released once the capture is complete. 
This poses the problem that the accelerometer data cannot be logged 
during this time, which is very important for estimating the blur kernel
. To overcome this, we use the async mode of camera capture(add ref to
async camera capture website). In this mode, the camera is initialized
for capture and it returns to the thread. Meanwhile, a busy flag is set
and accelerometer data is recorded. When this is done, the busy flag is
unset, which stops the data logging. The code snippet is given below

\begin{singlespacing}
\begin{lstlisting}[style=sharpclisting]
public async void capture(bool get_preview, bool register)
{
    // Get image preview
    if (get_preview == true)
    {
        _camera.GetPreviewBufferArgb(preview_image);
    }
    // Take a picture. Flag busy meanwhile.
    cam_busy = true;
    // If register mode is enabled, sleep for 500ms.
    if (register == true)
    {
        await System.Threading.Tasks.Task.Delay(500);
    }
    await _camsequence.StartCaptureAsync();
    cam_busy = false; // Done capture. Unset busy.
    transmit = true;
    imstream.Seek(0, SeekOrigin.Begin);
}
\end{lstlisting}
\end{singlespacing}

If an exposure time of 200ms is used, 20 accelerometer values are 
expected with a 10ms timer. However, close to 70 readings are obtained.
This is due to the transfer time of the image data also included in the
capture time. To overcome this problem, a fast moving image of a single
point light source is taken. Frames of 20 out of 70 samples of the
accelerometer data is used for estimating the best start and end times.
We use these start and end times for all the future kernel estimates.
%% We could add an image for reference here.

% Sending data over TCP
\subsection{Sending data over TCP}
\label{device:device_app:tcp}
The TCP communication enables a communication stream between the mobile
device and the computer. With the stream open, it can be treated as a 
continuous information channel, with no prior information about the size
of the data being sent. To works with this \emph{asynchronous} type of
data transmission, we encapsulate the data in keywords which is known to
the computer. This would boil down the problem to searching for the 
starting and ending keyword for each data type received. For example, if
we are sending a new image, we would use the keywords \textbf{STIM} and
\textbf{EDIM} to encapsulate the image data. An example: 

\begin{singlespacing}
\begin{lstlisting}[style=sharpclisting]
    // app_comsocket is the object representing the TCP connection.
    // app_comsocket.Send is used to send data over the connection.
    app_comsocket.Send("STIM\n");  // Start token
    app_comsocket.Send(imdata);     // Image data
    app_comsocket.Send("EDIM\n");  // End token
\end{lstlisting}
\end{singlespacing}

% Host side application
\section{Host side application}
\label{device:host}
Most of the host side application is written in python. The reason for 
choosing python is the vast support available for scientific computing,
communication protocols, image processing support and above all, python
being free software. However, some of the deblurring codes and 
super resolution are in Matlab.

The communication from mobile to computer is done using the inbuilt
\emph{socket} module in python. As mentioned in 
\ref{device:device_app:tcp}, the data from the mobile is sent
encapsulated in keywords. These keywords are used to parse the incoming
data and split it accordingly. 

\pagebreak

%----------- Deblurring ------------------------------------------------
\chapter{DEBLURRING}
\label{chap:deblurring}
As mentioned in \ref*{basic_theory:deconv}, blurring due to motion is
not desirable and we wish to remove it. In this section, we will look at
non blind and semi blind deconvolution techniques. 
\pagebreak
%----------- Depth estimation ------------------------------------------
\chapter{DEPTH ESTIMATION}
\label{chap:depth_estimation}
When taking picture, due to the variable depth of the scene, the amount 
of blur induced due to defocus or motion of the camera is different. 
When estimating depth, we use this fact that one of the parameters of 
the image is sensitive to depth. We particularly look at two types of 
depth estimation, namely depth from motion blur and shape from focus.

\section{Depth from motion blur}
\label{depth_estimation:motion}
Section \ref{basic_theory:image_blur:motion} gives a brief discussion 
about how every point in the image gets blurred by a different blur 
kernel due to the depth profile. If we neglect the parallax error, we 
can see that the only variation in the blur kernel is the scale. Hence,
estimating the depth profile boils down to estimating the scale of the 
blur kernel at different points. 

In this setup, we capture two images, one with very short exposure and
another with long exposure. The short exposure image is captured by
copying the image buffer. While a better choice would be to take capture
an image with very short exposure, it will tend to slow down the process
and induce more delay between the two images. 

Further, along with the two images, the motion of the camera is 
estimated by logging the accelerometer values. Thus, we have the latent
image, the blurred image and the blur kernel. We need to estimate the 
scale of the blur kernel at each point. Prior to estimating the depth,
it is very important that the two images are aligned. We would show that
misalignment could be a major reason for wrong depth profiles. However,
for small shifts, we found that image registration using cross 
correlation gave good results

To estimate the depth at each point, we iterate through various blur
kernel sizes and calculate the difference between the blurred image and
the blurred latent image for each scale of the kernel. If the scale is
correct at a given position, the error at that point will be the least
and that would be assigned as the depth value. Hence, we can formulate
our algorithm as
\begin{align*}
k(x,y) = argmin_d\{|im(x,y)_{blurred}-h(x,y,d)*im_{latent}(x,y)|^2\}
\end{align*}
Where $h(x,y,d)$ is the kernel with scale value of $d$. While this works
for the ideal case, the real depth map is very noisy. Hence, we low pass
filter the error map with a box filter. This algorithm works very well
when there is good texture in the image. Further, it is necessary that
the kernel is very accurate, else the depth map will go wrong. 

To verify our algorithm, we first tried estimating depth map in a 
sythetic case. We blurred an image with a known depth map
and ran our algorithm. The following were the results obtained.
\begin{figure}[htpb]
\begin{center}
\resizebox{50mm}{!} {\includegraphics *{images/depth/eg0/preview_im.png}}
\resizebox{50mm}{!} {\includegraphics *{images/depth/eg0/saved_im.png}}
\caption{Latent image and synthetic lurred image pair.}
\resizebox{50mm}{!} {\includegraphics *{images/depth/eg0/depth.png}}
\resizebox{50mm}{!} {\includegraphics *{images/depth/eg0/imdepth.png}}
\caption {True depth map and estimated depth map}
\label{fig:depth_synthetic}
\end{center}
\end{figure}\\
The results verified that our was useful for estimating depth from 
a scene. We then tried to test our algorithm for real cases. In our
 experiment, we found that the kernel can go to a maximum size of
$20\times20$ and hence we scaled the kernel size from $1\times1$ to 
$20\times20$. We used a filter of size $32\times32$ for the error map.
Some of the experimental results have been given below. 
\begin{figure}[ht]
\begin{center}
\resizebox{50mm}{!} {\includegraphics *{images/depth/eg1/preview_im.png}}
\resizebox{50mm}{!} {\includegraphics *{images/depth/eg1/saved_im.png}}
\caption{Latent image and Blurred image pair. This is an example of 
close up shot.}
\resizebox{50mm}{!} {\includegraphics *{images/depth/eg1/imdepth.png}}
\caption {Estimated depth map. Objects at the front are brighter}
\label{fig:depth_eg1}
\end{center}
\end{figure}\\
Observe that the image pair are very well aligned already and the object
is well textured. This made estimation of depth map accurate. However,
the following example is a failure case for estimating depth.
% Put a failure case here. Don't know which is a good idea.

To find out the reason for a bad depth map, we perturbed the kernel 
while estimating depth in synthetic case. It was found that if the 
images are misaligned or if the kernel is not accurate, the output depth
map was no accurate. % Need to think if we need to write about SSIM.

\section{Depth from focus}
\label{depth_estimation:focus}
Estimating depth from focus is termed as Structure From Focus (SFF). As
discussed in \ref{basic_theory:image_blur:optical}, varying a parameter
of the camera system changes the quality of the image at different depth.
We are interested in varying the focus of the camera to vary the 
\emph{sharpness} of the image. The sharpness of the image is measured 
using the focus operator. A very popular and easily implementable focus 
operator is the Sum Modified Laplacian (SML). The Modified Laplacian 
operator is defined as
\begin{align*}
\Delta_mI = |I*G_x| + |I*G_y|\\
G_x= [\begin{array}{ccc}-1 & 2 & -1\end{array}],  G_y = G_x^T
\end{align*}
The Sum Modified Laplacian operator does a average of the Modified
Laplacian over the neighbors. Thus,
\begin{align*}
\phi(x,y) = \sum_{i=-N}^{N}\sum_{j=-N}^{N}\Delta_mI(x-i,y-i)
\end{align*}
When a region is in focus, the SML value is the highest. The experiment
consists of taking 100 images with focus varying from minimum to maximum.
For each image, the SML operator is applied and the appropriate depth
for each pixel is when the focus operation peaks.

When changing the focus value, there is a scaling in the images. The 
robust way to correct this would be to register each image with the
first image using Fourier Mellin Transform or feature based image
registration. However, we found that finding the scale of the final 
image with respect to the first image and interpolating the scale for all
other images worked good. Further, this scaling was consistent across all
the experiments and hence finding the necessary scaling was a one time
operation. 

The following image pair is the first and the last image in the 100 images.
\begin{figure}[ht]
\begin{center}
\resizebox{60mm}{!} {\includegraphics *{images/focus/eg1/im3.png}}
\resizebox{60mm}{!} {\includegraphics *{images/focus/eg1/im99.png}}
\caption{First and the last image of the 100 images. Scale hasn't been
corrected.}
\label{fig:focus_first_last}
\end{center}
\end{figure}\\

To evaluate our algorithm, we used a filter size of $32\times32$. Further,
to confirm that our algorithm returns the correct depth map, we created
an all focus image from the set of images. The following are the obtained
results.
\begin{figure}[ht]
\begin{center}
\resizebox{100mm}{!} {\includegraphics *{images/focus/eg1/imdepth.png}}
\caption{Estimated depth map. Objects nearer are darker}
\resizebox{100mm}{!} {\includegraphics *{images/focus/eg1/imfocus.png}}
\caption{All focused image. }
\label{fig:depth_focus}
\end{center}
\end{figure}\\
From the depth map, we see that the objects have been segregated on the
basis of depth. Further, the areas of low texture don't respond very
well, as the SML operator is a high frequency measurement operator. 

\pagebreak
%----------- Image registration ----------------------------------------
\chapter{IMAGE REGISTRATION}
\label{chap:image_registration}
When taking multiple images, it is very likely that the subsequent 
images will be shifted and rotated. Consider the case of simple shift. 
In the absense of noise, we could use the normalized cross correlation
to estimate this shift. However, this method would fail when one of the 
image is blurred. A more sophisticated method would use feature matching
to get a good estimate of a the shift. However, the drawback is that it
is computationally intensive.

To overcome this problem, we can use the inertial sensor data on the 
mobile to get a good estimate of the change in orientation, since the 
inertial sensors give direct information about the motion during the 
photography period. 

Before we go into the case of pure translation, we need to understand 
the limitation of our current setup. We have a 3-axis accelerometer 
giving us the acceleration along ${x, y}$ and $z$ axes, relative to
the mobile frame of reference. Since our setup constrains that we have
only in-plane motion, the acceleration along $z$ axis does not give 
any information. Any generic motion in the plane can be represented by
three entities, ${t_x, t_y}$ and $R_z$. Hence, we have three unknowns
and only two equations from the two axes, which makes it an 
indeterminate system. Hence, we constrain the system by saying that
either the system goes through only translation or only rotation. Hence,
we have the following equations:

\begin{align*}
t_{x_i}=\int{\int{a_x}}\\
t_{y_i}=\int{\int{a_y}}\\
or\\
{\theta}_i=tan^{-1}({a_y}/{a_x})\\
\end{align*}

We look at two cases, one of pure translation and another of pure 
rotation.
\section{Pure translation}
\label{image_registration:pure_translation}
As mentioned in \ref{device:device_app:cam}, we can determine the 
trajectory of the device from the accelerometer data. To evaluate our
algorithm, we take a pair of images, one without blur and one with blur
with a 500ms duration between them. The accelerometer data will give a
rough estimate of the final position moved given by:

\begin{align*}
x_{\text{\emph{final}}}=\int_{t=t_{init}}^{t_{final}}
\int_{t=t_{init}}^{t_{final}}{a_xdt}\\
y_{\text{\emph{final}}}=\int_{t=t_{init}}^{t_{final}}
\int_{t=t_{init}}^{t_{final}}{a_ydt}
\end{align*}
The above final position is in mm and needs to be converted 
into pixel dimension. However, we don't have the distance of the scene
from the camera, which makes it an ill-posed problem. To overcome this, 
we iterate through various scales and find out the correct scale using 
RMS error. Hence,
\begin{align*}
(x_{shift}, y_{shift}) = argmin_k\{\sum_x\sum_y(im_1(x,y)
-im_2(x-k*x_{final},y-k*y_{final}))^2\}
\end{align*}
Note that the maximum shift is bound due to the finite duration. Hence,
our search space is drastically reduced from $360^o$ in the blind
case to a small sector(it is not a straight line due to the drift 
associated with the accelerometer readings) with the help of
accelerometer data. 

The following images are example outputs of the algorithm. While the overlap
is not exact, the finer registration can be done with more sophisticated
algorithm. 

\begin{figure}[ht]
\begin{center}
\includegraphics[width=300pt]{images/imreg/shift/eg1/imreg.png}
\caption{Example 1: Registered image pairs}
\includegraphics[width=300pt]{images/imreg/shift/eg2/imreg.png}
\caption{Example 2: Registered image pairs}
\end{center}
\end{figure}

\section{Pure rotation}
\label{image_registration:pure_rotation}
Assuming only in-plane rotation, we can relate the measured acceleration
to world acceleration (in the absense of other forces) as
\begin{align*}
a_x=gcos\theta\\
a_y=gsin\theta
\end{align*}
Where $\theta$ is the orientation of the device. Hence, the orientation
of the mobile can be estimated as
\begin{align*}
\theta=tan^{-1}(\frac{a_y}{a_x})
\end{align*}
As in the pure translation case, we take two images, one non-blurred and
another blurred, both separated by 500ms duration. We only rotate the 
device during this duration. Hence, we can use the acceleration values
estimated during this time frame to correct for the rotation. Note that
the process of rotation will also induce a small translation in the 
image. This makes the RMS error method of finding the appropriate
angle difficult. We hence use normalized cross correlation, which is 
independent of the shift. Thus,
\begin{align*}
% Bonkers. This does not make sense. 
\theta=argmax_t\big\{ifft(\frac{fft(im_1).fft(rotate(im_2,\theta_t))^*}
{|fft(im_1).fft(rotate(im_2,\theta_t))|})\}
\end{align*}
Where $fft(.)$ is the 2D discrete fourier transform, $ifft(.)$ is the 2D 
discrete inverse fourier transform and $rotate(im, \theta)$ rotates the
 image by $\theta$ degrees.

The following images serve as an example for the algorithm. In the first
and second example, the first image, which is taken with long exposure
is a highly blurred image. As is evident from the images, the algorithm
performs very good. The third image is a small rotation case and our
algorithm performs well for that case too.

\begin{figure}[ht]
\begin{center}
\includegraphics[width=300pt]{images/imreg/rotation/eg1/imreg.png}
\caption{Example 1: Very high blur and rotation}
\includegraphics[width=300pt]{images/imreg/rotation/eg2/imreg.png}
\caption{Example 2: Moderate blur and rotation}
\includegraphics[width=300pt]{images/imreg/rotation/eg3/imreg.png}
\caption{Example 3: Very small blur and rotation}
\end{center}
\end{figure}

\pagebreak
%----------- Image superresolution -------------------------------------
\chapter{IMAGE SUPERRESOLUTION}
\label{chap:image_superresolution}
Image super resolution involves taking multiple low resolution images
and combining them to get a single high resolution image. For example,
a set of four megapixel shots can be combined to get a single sixteen
megapixel shot. This is particularly useful because creating a sensor 
for a single sixteen megapixel camera is very costly. With introduction
of high performance microprocessors on mobiles, the burden of high 
resolution pictures can be shifted from the sensor to the computing side.

Super resolution can be broken down into two problems, Image registration
and Image restoration. The set of images need to be 
registered very accurately initially. Registration can be done in 
multiple ways, as was discussed in \ref{basic_theory:registration}. Note
that we need sub-pixel accuracy and hence a simple cross correlation 
method will not yield correct results. Information from accelerometer
obtained along with the set of images can prove very useful as it gives
the trajectory of motion, thus making accurate registration very easy. 
Due to lack of time, we could not verify the usefulness of the accelerometer
data. 

 Once the images are registered, the
super resolved image can be restored. Some of the methods include the
interpolation methods like the bicubic method, spline interpolation or 
even frequency methods. Simple methods like bicubic method would work 
in the case of very high accuracy registration. 

To show the proof of concept, we logged images continuously from the 
mobile phone at an interval of around 20ms. These images were then 
used for super resolving to a scale of 2. For this purpose, we used an
off the shelf code by Sune et al. To compare the results, we rescaled a
single image using bicubic interpolation. One example is presented 
below

\begin{figure}[H]
\begin{center}
\resizebox{120mm}{!} {\includegraphics *{images/super_resolution/imlr3.png}}
\caption{Low resolution image scaled using bicubic interpolation}
\resizebox{120mm}{!} {\includegraphics *{images/super_resolution/imsr3.png}}
\caption{Super resolved image}
\label{fig:super_resolution}
\end{center}
\end{figure}

From the figure, it is clear that the super resolution method gives a 
sharper output when compared to bicubic interpolation method. To quantify
the output, we measured the SSIM between the two images and found it to
be 0.997192, which is high, but says that the super resolved image contains
more information as compared to the bicubic method. 

\begin{figure}[H]
\begin{center}
\resizebox{120mm}{!} {\includegraphics *{images/super_resolution/diff.png}}
\caption{Disparity map of the two images}
\label{fig:super_resolution}
\end{center}
\end{figure}

However,
the code used was very slow. It took close to 10 minutes for calculating
the optical flow, necessary for registering the images for 5 images of 
size $240\times320$. The image restoration part was very fast. Due to
absence of additional data, the optical flow calculation method was not
fast enough. This part can be replaced by the information from the motion
sensors to speed up the code. 
\pagebreak

%----------- Future work -----------------------------------------------
\chapter{FUTURE WORK}
\label{chap:future_work}

\pagebreak

%----------- Conclusion ------------------------------------------------
\chapter{CONCLUSION}
\label{chap:conclusion}

\pagebreak

\end{document}

