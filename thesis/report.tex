% Use the IITM Dissertion class
\documentclass[BTech]{iitmdiss}
\usepackage{times}
\usepackage{t1enc}
\usepackage{float}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{verbatim}
\usepackage[hypertex]{hyperref} % hyperlinks for references.
\usepackage{amsmath} % easier math formulae, align, subequations \ldots

% Enable code snippets coloring
\usepackage{listings}
\usepackage{xcolor}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

% Python details
\lstdefinestyle{pylisting}{frame=none,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true
  tabsize=2
}

% C# details
\lstdefinestyle{sharpclisting}{frame=none,
  language=[Sharp]C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true
  tabsize=2
}

\lstset{language=Python, frame=none}
\lstset{language=[Sharp]C, frame=none}

\begin{document}
%----------- Title page ------------------------------------------------
\title{Utilizing Motion Sensor Data For Some Image Processing Applications}
\author{\hspace{35pt} Saragadam R V Vishwanath \newline EE10B035}
\date{MAY 2014}
\department{ELECTRICAL ENGINEERING}
\maketitle
\pagebreak

%----------- Certificate -----------------------------------------------
\certificate

\vspace*{0.5in}

\noindent This is to certify that the thesis titled {\bf Utilizing Motion
Sensor Data For Some Image Processing Applications}, submitted by
  {\bf Saragadam Raja Venkata Vishwanath}, 
  to the Indian Institute of Technology, Madras, for
the award of the degree of {\bf Bachelor of Technology}, is a bona fide
record of the research work done by him under our supervision.  The
contents of this thesis, in full or in parts, have not been submitted
to any other Institute or University for the award of any degree or
diploma.

\vspace*{1.5in}

\begin{singlespacing}
\hspace*{-0.25in}
\parbox{2.5in}{
\noindent {\bf Prof. A. N. Rajagopalan} \\
\noindent Research Guide \\ 
\noindent Professor \\
\noindent Dept. of Electrical Engineering\\
\noindent IIT-Madras, 600 036 \\
} 
\hspace*{1.0in} 
\end{singlespacing}
\vspace*{0.25in}
\noindent Place: Chennai\\
Date: 12th May 2014

\pagebreak

%----------- Acknowledgement -------------------------------------------
\acknowledgements

I would like to express my heartfelt gratitude to my guide, 
Dr. A. N. Rajagopalan for his invaluable support and guidance throughout
my project duration. I have gained immense insight into computational
photography and image processing in this one year, which I believe will 
hold me in good stead in my future endeavors. I am indebted to him for
patiently sitting with me at times when I could not see any path forward. 
I will always remember him as the professor who said ``There is nothing
sacred about time or frequency''.

I also thank all my lab mates for being extremely helpful and
for their patience and guiding me through difficult tasks. I had the
privilege of working with some of the best minds in the department.

I would also like to thank my parents for being flexible and giving me moral
support throughout my education. I believe that what I am today and what
I will be tomorrow is a result of their wonderful upbringing. I  am also
thankful to my brother, who constantly updates me with the latest trends
in technology and comes forward to give constructive critisism on my
programs. His hobby of photography has also proved very useful in 
understanding some of the nitty-gritties of my project.

Last but not the least, my wing mates have been the best 
experience in my undergraduate life. Living with fifteen different minds
with always something new to learn from each person, I have learnt more
than I ever learnt anything before coming here. They have been with me
throughout my undergraduate studies, with healthy debates, joyful times,
tough exams and quintessential assignments. I owe a lot to them.

% Need to write my name on the right side.
Vishwanath

%----------- Abstract --------------------------------------------------
\abstract
We explored various image restoration and registration techniques using 
data obtained from a mobile device. The first one was non-blind image
deblurring using data obtained from motion sensors. We wanted to evaluate
the usefulness of having extra information from the motion sensors, which
give a clue about camera motion. We could only prove marginally good 
results. However, we showed that deconvolution with our kernel as the 
intial estimate either converged quicker or produced good results for low
texture images. The second problem we addressed was that of depth
estimation using blurred and latent image pair method and depth from
focus. In the former problem, we showed that with a good estimate of 
blur kernel from motion
sensors, depth can be estimated very efficiently. The third problem 
involved evaluation of image registration algorithms using the information
from motion sensors. We showed that the registration results are very 
accurate and the registration can be done very fast and efficiently even
when one of the image in the image pair is blurred. At the end, we 
evaluated some of the off-the-shelf multi-frame image super resolution
 algorithms with data obtained from the mobile. Not much time was spent on
this part of the project. However, we showed that the super resolution
algorithms, though slow, produced better results than bicubic interpolation

%----------- Table of content ------------------------------------------
\begin{singlespace}
\tableofcontents
\thispagestyle{empty}

\listoffigures
\addcontentsline{toc}{chapter}{LIST OF FIGURES}
\end{singlespace}
\pagebreak

%----------- Abbrevations ----------------------------------------------
\abbreviations

\noindent 
\begin{tabbing}
xxxxxxxxxxx \= xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \kill
\textbf{IITM}   \> Indian Institute of Technology, Madras \\
\textbf{WP8}    \> Windows Phone 8 \\
\textbf{FFT}    \> Fast Fourier Transform \\
\textbf{DFT}    \> Discrete Fourier Transform \\
\textbf{SSIM}   \> Structural Similarity Measure \\
\textbf{PSF}    \> Point Spread Function \\
\textbf{LPF} 	\> Low Pass Filter \\
\end{tabbing}

\pagebreak

%----------- Notations -------------------------------------------------
\chapter*{\centerline{NOTATION}}
\addcontentsline{toc}{chapter}{NOTATION}

\begin{singlespace}
\begin{tabbing}
xxxxxxxxxxx \= xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \kill
\textbf{$im(x,y)$}  \> Image intensity at location (x, y) \\
\textbf{$\delta(x,y)$}  \> The discrete time 2D dirac delta function \\
\textbf{$im{\Leftrightarrow}IM$} \> $IM$ and $im$ are the fourier transform 
pairs, with $im$ being the spacial domain\\
 \> representation and $IM$ being the frequency domain representation.\\
\textbf{$|.|^2$} 	\> L2 norm squared of the vector\\
\textbf{$E|.|^2$} 	\> Expectation value of the L2 norm
\end{tabbing}
\end{singlespace}

\pagebreak

% Start numbering the pages from now.
\pagenumbering{arabic}

%----------- Introduction ----------------------------------------------
\chapter{INTRODUCTION}
\label{chap:intro}
There has been a recent shift in computation from traditional computers
to ubiquitous mobiles. With the ever increasing power of the mobiles, 
executing many of the image processing algorithms on mobile has been 
made easy. Further, the many peripherals available for the mobile, like
the inertial sensors, controllable focus, TCP, bluetooth etc., 
computational photography is greatly advanced. 

Using the mobile platform for computational photography has the unique
advantages of capturing motion during exposure time, which can be used
for estimating the blur kernel or estimating the direction of motion
for reducing the search space of image registration. Further, 
controllable focus and the ability to capture fast multiple images means
that it is easy to implement multichannel super resolution or deblurring
algorithms. 

We wanted to evaluate some of the important algorithms in computational
photography by using data obtained from the mobile. In particular, we 
wanted to verify that the motion sensors greately enhance the efficiency
and accuracy of the algorithms. While we did not port the algorithms to
the mobile platform, we showed that the algorithms, due to their simplicity,
can be ported with great ease to the mobile. 

\section{Prior work}
\label{intro:prior_work}
The best known project involving the camera as an imaging device is the 
Camera 2.0 project by the Computer Science department at Stanford. The
project involved hacking the Nokia N900 firmware to write custom algorithms
for focus. Further, the lab also built a custom camera called Frankencamera
which runs linux and is completely programmable, making it highly flexible.
The paper by \cite{vsindelavr2013image} discusses the usage of a smart
phone for deblurring images which have been degraded by rotational blur.

While computational imaging on a mobile platform is relatively new, there
has been a lot of work on combining motion sensors with DSLR to perform
accurate deblurring. \cite{joshi2010image} Combines a 3 axis accelerometer
and 3 axis gyroscope with a DSLR. This paper is one of the main motivation
for the part of image deblurring. \cite{bae2013accurate} builds upon
\cite{joshi2010image} and explains how blind PSF estimation can be
combined with motion sensors data to improve deblurring. 

\section{Report layout}
\label{intro:layout}
The project report is arranged in the following manner. The chapter on 
\textbf{BASIC THEORY} presents a brief background of the various algorithms
and the mathematics involved in the project. \textbf{THE DEVICE} gives an
in-depth account of the mobile platform, the application written for the
mobile platform and the host side software. \textbf{DEBLURRING} 
explains the implementation of the deblurring algorithm using the data
obtained from the mobile. \textbf{IMAGE REGISTRATION} discusses about how
inertial sensor data can be used to reduce the search space of image 
registration process. \textbf{DEPTH ESTIMATION} has two parts, 
\emph{Depth estimation using motion blur} and \emph{Depth estimation 
using focus measure}. Finally, \textbf{SUPER RESOLUTION} looks at how 
multiple frames can be used to super resolve an image.

At the end of the report, we give a brief discussion the future of the 
project and how the algorithms can be taken forward. We concluded that
though some of the experiments were not successful, they gave very
interesting results, which pave way for future work.

%----------- Basic Theory ----------------------------------------------
\chapter{BASIC THEORY}
\label{chap:basic_theory}
This chapter will go through some of the basics which were crucial 
throughout the project and which will be a primer to all the future 
chapters.

\section{Position from acceleration data}
\label{basic_theory:accel}
A major part of the project involves estimating the trajectory of motion
from accelerometer data. In the current context, we assume that we have
access to a 3-axis accelerometer alone. The 3-axis accelerometer 
measures physical acceleration, which is a combination of the 
acceleration due to gravity and any external force. Further, the 
measured values are relative to the mobile frame of reference. Hence, 
we can model our measured acceleration as,
\begin{align*}
\hat{a}_x= g_x + a_x + n_x\\
\hat{a}_y= g_y + a_y + n_y\\
\hat{a}_z= g_z + a_z + n_z
\end{align*}
Where $n_x, n_y$ and $n_z$ are noise in measurements.For calculating
 the trajectory, we need $\{a_x,a_y,a_z\}$. To obtain 
these values, we need to subtract the acceleration due to gravity. 
However, we have no further information apart from the above equations.
To get a good estimate of the required acceleration, we assume that the
gravity vector is quasi-static when there is only translation in the 
mobile device. Hence, we estimate our gravity vector as,
\begin{align*}
g_x^t= \alpha{g_x^{t-1}} + (1-\alpha)\hat{a}_x^t\\
g_y^t= \alpha{g_y^{t-1}} + (1-\alpha)\hat{a}_y^t\\
g_z^t= \alpha{g_z^{t-1}} + (1-\alpha)\hat{a}_z^t
\end{align*}
Where $\alpha$ is the LPF constant and lies between 0 and 1. 
Once we have the device acceleration, the equation for finding the 
trajectory of motion would be,
\begin{align*}
x(t)=\int_0^{t}\int_0^{t}{a_x(t)dt}\\
y(t)=\int_0^{t}\int_0^{t}{a_y(t)dt}\\
z(t)=\int_0^{t}\int_0^{t}{a_z(t)dt}
\end{align*}
The discrete time approximation is,
\begin{align*}
x[n]=({\sum_{t}^{n}}{\sum_{t}^{n}}a_x[n]){\times}T^2\\
y[n]=({\sum_{t}^{n}}{\sum_{t}^{n}}a_y[n]){\times}T^2\\
z[n]=({\sum_{t}^{n}}{\sum_{t}^{n}}a_z[n]){\times}T^2
\end{align*}
Though this gives a good estimate of the orientation of the device when
there is no motion or the device acceleration when there is only
translation, the method suffers from two drawbacks:
\begin{enumerate}
\item The noise in measurement creates a random walk pattern. This might
not be of major concern as it would only create a small disturbance in 
the over all trajectory.
\item If the estimate of gravity vector is not correct, it would create
a DC offset to the device acceleration. This gives rise to drift, which
blows up as $t^2$ which can give rise to wrong trajectory.
\end{enumerate}
The following graph explains how the drift can be of serious concern when
estimating the trajectory. 

\begin{figure}[H]
\begin{center}
\resizebox{150mm}{!} {\includegraphics *{images/drift_image.png}}
\caption {Plot showing the effect of DC offset to acceleration value.
The drift was iterated from -1\% to 1\% of the maximum acceleration 
value.}
\label{fig:drift_example}
\end{center}
\end{figure}

Due to lack of information regarding the orientation of the mobile, which
gives rise to a bad estimate of gravity vector, the drift will exist 
and make the estimate unreliable. In the forthecoming chapters, we will
see the consequences of have a bad trajectory and possible ways to 
rectify the problem.

\section{Image blur}
\label{basic_theory:image_blur}
Image blur is equivalent to low pass filtering in 1D signals. While 
image blur is desired in some applications like animation, it is 
undesirable when taking still photos. Two types of blurs are of interest
in this project, namely defocus blur and motion blur.

\subsection{Defocus blur}
\label{basic_theory:image_blur:optical}
Defocus blur is caused when the image plane does not match with the 
optical plane of the object. This arises only in real aperture cases
when the focal length of the lens is finite and objects are at varied
distances. 

The blur that arises due to defocus can be modeled as a 2D gaussian
filtering operation. Hence, the point spread function can be modeled as,
\begin{align*}
h(x,y) = \frac{1}{2\pi\sigma^2}e^\frac{-(x^2+y^2)}{2\sigma^2}
\end{align*}
Where $\sigma$ depends on the depth of the object from the lens. For a 
given lens to sensor distance, $\sigma$ increases with increasing depth.

The defocus blur can be used in the shape from focus method, which 
varies one particular parameter of the system like the lens position or
the object position to bring the object in focus. The idea is to have a 
\emph{focus measure}. \cite{pertuz2013analysis} gives a good outline of 
various focus measures used commonly. Further discussion about shape from focus will be carried out in the chapter on Depth Estimation.

\subsection{Motion blur}
\label{basic_theory:image_blur:motion}
Motion blur occurs due to relative motion between the scene and the 
camera during the exposure time. This could mean the object in the scene
is moving or there is a camera shake. This report deals with the problem
of camera shake.

The process of camera shake can be considered as an averaging of 
multiple sharp images shifted due to the camera motion. Hence for a 
planar scene, 
\begin{align*}
\hat{im}(x,y) = \sum_tim(x-x_t,y-y_t)\\
\implies\hat{im}(x,y) = \sum_t\delta(x_t, y_t)*im(x,y)
\end{align*}
where $\delta(x_t, y_t)$ is an impulse at $(x_t, y_t)$ and $\hat{im}$ is
the observed image.

This can be rewritten as,
\begin{align*}
\hat{im}(x,y) = (\sum_t\delta(x_t, y_t))*im(x,y)\\
\implies \hat{im}(x,y) = h(x,y)*im(x,y)
\end{align*}

$h(x,y)$ is called the point spread function. If the image is not a 
planar scene, then every point in the scene moves by a different amount
due to the camera shake. If we assume that there is no parallax error,
we have,
% Do we need to write about why they move by different lengths?
\begin{align*}
\hat{im}(x,y) = \sum_t\delta(x_t.k(x,y), y_t.k(x,y))*im(x,y)
\end{align*}
where $k(x,y)$ is the depth of the image at $(x,y)$. This space variant
blurring operation can be used to calculate depth information from the 
images, as objects at different depths will be blurred to a different
extent. Calculating $k(x,y)$ gives the depth map of the image.

\section{Image deconvolution}
\label{basic_theory:deconv}
The process of removing the blur in the image is known as image
deconvolution. Deconvolution can be broadly of two types, blind and non-
blind. If the psf is known apriori, it is known as non-blind 
deconvolution and if it not know, it is called blind deconvolution. 
Blind deconvolution is an ill-posed problem, as any combination of 
latent image and psf can give rise to the observed image. Hence, various
constraints are used when trying to retrieve the latent image\cite{fergus2006removing, krishnan2009fast, levin2007deconvolution,gupta2010single}.For example, an image can be considered as piecewise smooth function with
sharp images. Hence, we can try to reduce the total variance of the 
obtained latent image\cite{money2006total,chan1998total}. If the blur
is caused due to a straight line shaped PSF, analyzing the frequency
domain of the image can help retrieving the PSF\cite{oliveira2007blind}.

On the other hand, non-blind deconvolution is less ill posed than the
blind case, as we have the psf estimate. As discussed in the previous 
section, the blurred image can be represented as the convolution of 
the psf and the latent image in case of a planar scene. This 
transforms into multiplication in the frequency domain. Hence, we can
simply divide the image's fourier transform with the psf's fourier
transform to get the original image. i.e,
\begin{align*}
IM_{latent} = \frac{\hat{IM}}{PSF}
\end{align*}
Where $IM$ is the discreet fourier transform of $im$. However, the 
observed image is further degraded by noise. Hence, the actual model of
the observed image is
\begin{align*}
\hat{im}(x,y) = h(x,y)*im(x,y) + n(x,y) 
\end{align*}
Where $n(x,y)$ is the noise. Hence, the equation in the frequency domain
can be loosely expressed as
\begin{align*}
\hat{IM} = H.IM + N\\
IM_{latent} = \frac{\hat{IM}}{H}\\
\implies IM_{latent} = IM + \frac{N}{H}
\end{align*}
Note that $h(x,y)$ has a low frequency structure and noise has a high 
frequency structure. Hence, the division will blow up the calculated 
image. 

\subsection*{Wiener deconvolution}
\label{basic_theory:deconv:wiener}
Instead of simple division, we reformulate our problem as finding a 
filter which when convolved with the observed image would return the
latent image. Further, we wish to reduce the error between the latent
image and the image obtained by filtering the observed image, i.e
\begin{align*}
minimize \hspace*{4mm} E|im(x,y)-f(x,y)*\hat{im}(x,y)|^2
\end{align*}
Where $f(x,y)$ is the desired filter and $E|.|$ is the expectation
operator. Changing to frequency domain, we
have
\begin{align*}
minimize \hspace*{4mm} E|IM-F.\hat{IM}|^2\\
\hat{IM}=IM.H + N
\end{align*}
Differentiating with respect to $F$, we get
\begin{align*}
F = \frac{H^*}{|H|^2 + \alpha}\\
\implies IM_{latent} = \hat{IM}.F\\
=\frac{H^*.\hat{IM}}{|H|^2 + \alpha}
\end{align*}
Where $\alpha$ is the inverse signal to noise ratio of the image.
% Very vague. Is that ok?

\subsection{Regularized deconvolution}
\label{basic_theory:deconv:reg}
% bleh, I have no clue what to write
Since we know that the image blows up at the edges, we can impose a 
penality on the edge weights. Let $g_x$ and $g_y$ be the 
horizontal and vertical differentiation filters . Then, we need
\begin{align*}
minimize \hspace*{4mm} |y-h*x|^2 + \alpha|g_x*x|^2 + \alpha|g_x*x|^2\\
\implies minimize \hspace*{4mm} |Y-HX|^2 + \alpha|G_xX|^2 + \alpha|G_xX|^2\\
\end{align*}
Where $X$ represents the DFT of $x$. Differentiating the above equation
with respect to $X$, we get
\begin{align*}
X=\frac{H^*Y}{|H|^2+\alpha(G_x^2+G_y^2)}
\end{align*}

This is called regularized deconvolution. Apart from the simple deblurring
algorithms, there are other deconvolution methods, which work by reducing
a certain cost function. Literature on some of these can be found at
\cite{longrichardson,wang2009robust,bioucas2006total}

\section{Image registration}
\label{basic_theory:registration}
Image registration finds application in many areas like medical imaging,
aerial photography and automated target recognition. In computational
photography, image registration is used in wide number of areas like panorama
stitching, depth estimation using stereo vision and video stabilization.

Image registration involves finding the transform between a pair of 
images. The transform could be a simple translation and rotation or a
complicated warping. For simple cases like translation only model or 
rotation only model, many of the frequency based methods give good 
results. For advanced models, feature based registration is used. The 
SIFT based image registration, for example, identifies key points in
each image. Once we have the corresponding points in both the images,
the problem is of estimating the matrix that created this transform.

Note that when there is not much information available or we have a 
blurred and latent image pair, registration becomes difficult. Section
\ref{chap:image_registration} has discussion about how this can be 
simplified using data from accelerometer. In this section, we will look
at a simple case of registering image pair which are transformed by shift
alone. % And rotation and scale? It would be good because then we can
% compare it in the image registration section.

\subsection{Image registration using cross correlation}
Let $im_1$ and $im_2$ be the image pairs. Since we are looking at a 
translation only model, we have,
\begin{align*}
im_2(x,y)=im_1(x-x_0, y-y_0)
\end{align*}
Given these two images, we wish to estimate $(x_0,y_0)$. A shift in the
spacial domain reflects as a phase multiplication in the frequency domain.
Hence, if $IM_1$ is the DFT of $im_1$, then,
\begin{align*}
IM_2(u,v) = IM_1(u,v)e^{-j(\frac{2{\pi}x_0u}{M}+\frac{2{\pi}y_0u}{N})}
\end{align*}
Let,
\begin{align*}
H(u,v) = \frac{IM_1(u,v){\times}IM_2^*(u,v)}{|IM_1(u,v){\times}IM2(u,v)|}\\
\implies H(u,v) = e^{-j(\frac{2{\pi}x_0u}{M}+\frac{2{\pi}y_0u}{N})}
\end{align*}
This gives,
\begin{align*}
h(x,y) = \delta(x_0, y_0)
\end{align*}
Hence, the location of peak in h(x,y) will give the shift between the 
two images. 

\pagebreak
%----------- The Device ------------------------------------------------
\chapter{THE DEVICE}
\label{chap:device}
Primarily, we want the device to function as an acquisition system which
would take pictures and measure some sensor values. While choosing the
mobile platform, we wanted to look at the following
features
\begin{itemize}
\item The device should be low cost. We are looking at a mobile which
would not cost above Rs. 15,000. This gave us the option to choose 
Android mobiles or the low end Windows Phone Mobiles.
\item The device should have a good Software Development Kit. Android
and Windows Phone both have a very good SDK.
\item The device should have accelerometer, gyroscope and manual focus
ability. Almost all the mobiles which come today have these features 
and hence was not very critical.
\item The device should have easy image handling capabilities. Here, 
Windows Phone was a clear winner due to the powerful Nokia Imaging SDK.
\item The device should have TCP/Bluetooth capability. Again, every 
smart phone available today has these features. 
\end{itemize}
 
With the above points in mind and considering the low cost, we chose to
go for Nokia Lumia 520 (henceforth called 520) which features a 5 MP
camera, 3-axis accelerometer, WiFi and Blutooth support. However, the 
tradeoff was that we could not get a gyroscope on the mobile. This 
would be a serious hinderance for handling rotation of the mobile along
with translation. However, since we were only evaluating the 
possibilities of computational photography on the mobile platform, we
chose to ignore it and see what can be done with the accelerometer 
alone. 

From the ease of usage perspective, the Windows Phone 8 SDK has a very
simple interface with coding in C\# and GUI design using XAML script. 
Visual Studio makes it very easy to create applications (apps) fast. A
number of code examples for developing WP8 apps are available at \cite{wp8:windowsphone8}

% Device side application
\section{Device side application}
\label{device:device_app}
The app is mainly used for logging information to the computer so that
algorithms can be tested offline. The major advantage of this method is
that it gives the power of the computer along with the flexibility to 
experiment with various methods. 
 
To make the application versatile and useful for future development, we
added multiple features to capture images. A screenshot of the 
application is given below.
\begin{figure}[htpb]
    \begin{center}
        \resizebox{100mm}{!} {\includegraphics *{images/app_screenshot.png}}
        \caption {A screenshot of the application}
        \label{fig:app_screenshot}
    \end{center}
\end{figure}
The top right, \textbf{Information block} serves as a debugging block 
during the image capturing process. For example, when taking images with
long exposure, it indicates the number of samples of accelerometer data
obtained. The \textbf{Debug section} box indicates the focus position and other
debug information. The focus position can be controlled by the yellow
slide bar which is situated at the bottom of the application. 

The \textbf{Accelerometer} box indicates the
current value of the accelerometer. The button titled \textbf{Click} is 
used for capturing a single long exposure image. \textbf{Connect} button
is used for connecting to the computer using the TCP protocol. The Host
and the port can be selected in the fields at the right bottom of the 
application screen. Apart from these features, the following features 
add versatility to the application

\begin{itemize}
    \item \textbf{Log images} enables continuous logging of the image 
    frames to the computer when TCP stream is open. With the default
    setting, the duration between two frames is close to 20ms.
    \item \textbf{Enable delay} adds a 500 ms delay between the image
    retrieved from the image buffer and the long exposure shot.
    \item \textbf{Enable preview} captures an image from the image buffer
    (which can act as a latent image) prior to a long shot image. 
    \item \textbf{Start sensor log} enables continous logging of the 
    accelerometer data to the computer when the TCP stream is open. 
    \item \textbf{Focus sweep} records 100 images with varied focus 
    distance and sends them to the computer.
\end{itemize}

The application has a 10ms timer, which takes care of all the logging 
and image capture functions. While it is more apt to use a separate 
thread, we found that using a timer was an easy method. 

We discuss some of the methods implemented for logging the data and 
sending it over TCP to understand the intricacies involved.

% Logging accelerometer data during exposure
\subsection{Logging accelerometer data during exposure}
\label{device:device_app:cam}
When the \textbf{Click} button is toggled, a camera capture sequence
is started, which is programmed to take a picture with an exposure time
of 200ms (adjustable). However, by default, this suspends all other 
thread activities and is only released once the capture is complete. 
This poses the problem that the accelerometer data cannot be logged 
during this time, which is very important for estimating the blur kernel
. To overcome this, we use the async mode of camera capture(add ref to
async camera capture website). In this mode, the camera is initialized
for capture and it returns to the thread. Meanwhile, a busy flag is set
and accelerometer data is recorded. When this is done, the busy flag is
unset, which stops the data logging. The code snippet is given below

\begin{singlespacing}
\begin{lstlisting}[style=sharpclisting]
public async void capture(bool get_preview, bool register)
{
    // Get image preview
    if (get_preview == true)
    {
        _camera.GetPreviewBufferArgb(preview_image);
    }
    // Take a picture. Flag busy meanwhile.
    cam_busy = true;
    // If register mode is enabled, sleep for 500ms.
    if (register == true)
    {
        await System.Threading.Tasks.Task.Delay(500);
    }
    await _camsequence.StartCaptureAsync();
    cam_busy = false; // Done capture. Unset busy.
    transmit = true;
    imstream.Seek(0, SeekOrigin.Begin);
}
\end{lstlisting}
\end{singlespacing}

If an exposure time of 200ms is used, 20 accelerometer values are 
expected with a 10ms timer. However, close to 70 readings are obtained.
This is due to the transfer time of the image data also included in the
capture time. To overcome this problem, a fast moving image of a single
point light source is taken. Frames of 20 out of 70 samples of the
accelerometer data is used for estimating the best start and end times.
We use these start and end times for all the future kernel estimates.
%% We could add an image for reference here.

% Sending data over TCP
\subsection{Sending data over TCP}
\label{device:device_app:tcp}
The TCP communication enables a communication stream between the mobile
device and the computer. With the stream open, it can be treated as a 
continuous information channel, with no prior information about the size
of the data being sent. To works with this \emph{asynchronous} type of
data transmission, we encapsulate the data in keywords which is known to
the computer. This would boil down the problem to searching for the 
starting and ending keyword for each data type received. For example, if
we are sending a new image, we would use the keywords \textbf{STIM} and
\textbf{EDIM} to encapsulate the image data. An example: 

\begin{singlespacing}
\begin{lstlisting}[style=sharpclisting]
    // app_comsocket is the object representing the TCP connection.
    // app_comsocket.Send is used to send data over the connection.
    app_comsocket.Send("STIM\n");   // Start token
    app_comsocket.Send(imdata);     // Image data
    app_comsocket.Send("EDIM\n");   // End token
\end{lstlisting}
\end{singlespacing}

% Host side application
\section{Host side application}
\label{device:host}
Most of the host side application is written in python. The reason for 
choosing python is the vast support available for scientific computing,
communication protocols, image processing support and above all, python
being free software. However, some of the deblurring codes and 
super resolution are in Matlab.

The communication from mobile to computer is done using the inbuilt
\emph{socket} module in python. As mentioned in 
\ref{device:device_app:tcp}, the data from the mobile is sent
encapsulated in keywords. These keywords are used to parse the incoming
data and split it accordingly. 

A number of functions have been written as part of host side software
for data reception, interpretation and image processing. Some of the 
main modules will be discussed in this section

\subsection{TCP data handling}
\label{device:host:tcp}
The TCP data handling, as the name suggests is responsible for receiving
data from the mobile, which includes images and motion sensors data. The
main functions of this module involve receiving blur and latent image 
pair, continuously receiving images and continuously receiving motion
sensor data. For example, in order to receive a blurred and latent image
pairs, we need to execute the function \verb|get_tcp_data()|, which
in turn executes the following

\begin{singlespacing}
\begin{lstlisting}[style=pylisting]
# Listen to the TCP socket and get data.
dstring = tcp_listen();
# Parse and save the data obtained from TCP		
save_data(dstring);			
# Return a handle to the data, which consists of images and the motion sensor data.
return TCPDataHandle(dstring) 
\end{lstlisting}
\end{singlespacing}

To do a continous reception of images, we use the following function 
calls.

\begin{singlespacing}
\begin{lstlisting}[style=pylisting]
# All the images are encapsulated in tokens. Further, all tokens are
# Separated by the null character.
strt_token = 'S\x00T\x00I\x00G\x00'
end_token = 'E\x00D\x00I\x00G\x00'
frame_token = 'S\x00T\x00I\x00M\x00'
fstart = 'STIM'
fend = 'EDIM'
# First, receive the data from the mobile.
continuous_recv(strt_token, end_token, frame_token, 'tmp/burst/tokens.dat')
# Once we have all the data, split it and save the first 100 images.
extract_images('tmp/burst/tokens.dat', 100, fstart, fend, 'tmp/burst/src')
\end{lstlisting}
\end{singlespacing}

\subsection{Visualizing sensor data}
\label{device:host:visu}
Another important feature of the host software is the ability to visualize
the sensor data dynamically. This is very useful for understanding the
attitude of the device and how the motion sensor data can be used for
operations like video stabilization. 

There are two main functions for visualizing the data. The first function
shows a continuous graph of the measured accelerometer value. A screenshot
of the graphs is attached below.

\begin{figure}[H]
\begin{center}
\resizebox{120mm}{!} {\includegraphics *{images/live_sensor_log.png}}
\caption{Screen shot of live sensor logging}
\label{fig:live_sensors}
\end{center}
\end{figure}

The second function is used for visualizing the attitude of the mobile
for in plane motion. With the availability of only the accelerometer,
we created a UI for showing the orientation and the jerk direction, 
which is obtained by the velocity vector, both corresponding to in plane
motion. The following is an example screenshot.

\begin{figure}[H]
\begin{center}
\resizebox{120mm}{!} {\includegraphics *{images/attitude_ui.png}}
\caption{Screen shot of attitude UI}
\label{fig:attitude_ui}
\end{center}
\end{figure}

\pagebreak

%----------- Deblurring ------------------------------------------------
\chapter{DEBLURRING}
\label{chap:deblurring}
As mentioned in \ref*{basic_theory:deconv}, blurring due to motion is
not desirable and we wish to remove it. In this section, we will look at
non blind and semi blind deconvolution techniques. 

Prior to deblurring, we need to estimate the blur kernel. As mentioned in
\ref{basic_theory:accel}, the accelerometer data can be used to estimate
the trajectory of the camera and thus the PSF can be estimated. However,
since we don't know the scene depth and there is a drift due to approximate
value of gravity vector, we iterate through various possible depths and
shifts to find the appropriate image. 

\section{Constructing the kernel}
\label{deblurring:kernel}
Initially, we wanted to verify that the trajectory obtained from the 
sensors can be used to construct a good kernel estimate. For this purpose,
we took a long shot image of a single point light source. The image 
obtained would represent the blur kernel. Hence, we could compare the 
kernel we constructed and the kernel from the image. An example image
and kernel pair have been presented below.
\begin{figure}[H]
\begin{center}
\resizebox{50mm}{!} {\includegraphics *{images/deblur/ground_truth1.png}}
\caption{Actual image showing the point spread function, which acts as
our ground truth measurement}
\resizebox{50mm}{!} {\includegraphics *{images/deblur/constructed1.png}}
\caption{Kernel constructed from sensors data. The scale is not appropriate.
Further, there is no drift compensation.}
\label{fig:kernel_ground_truth}
\end{center}
\end{figure}
From the above figures, we see that the motion sensors data gives a good
initial estimate for the PSF. Note that the scale hasn't been calculated
and the drift hasn't been compensated. The scale and the drift information
are available during the deblurring of the image. 

The above example was a good case, where there was little drift. The 
following example has a considerable drift, which gives rise to an 
erroneous kernel.
\begin{figure}[H]
\begin{center}
\resizebox{50mm}{!} {\includegraphics *{images/deblur/ground_truth2.png}}
\caption{Actual image showing the point spread function, which acts as
our ground truth measurement}
\resizebox{50mm}{!} {\includegraphics *{images/deblur/constructed2.png}}
\resizebox{50mm}{!} {\includegraphics *{images/deblur/im_0_004248_0_013347.png}}
\caption{Plain kernel and the kernel corrected for drift}
\label{fig:kernel_ground_truth}
\end{center}
\end{figure}
As is visible from the above figures, there is a significant drift, which
is visible in the first constructed image. We iterated through various
possible drifts and found visually that the drift correction was
$(0.004284, 0.013347)mm$, i.e, the final position is offset by this 
amount. 

\section{Deblurring the image}
\label{deblurring:deblurring}
Once we have the approximate kernel, we can perform deblurring. Since we
don't have the scene depth, we iterate through various depths. We assumed
that our kernel was very accurate and tried estimating the latent image
using a simple weiner filter. Some of the results, along with blind
deconvolution output by Jia et. al are given below.

\begin{figure}[H]
\begin{center}
\resizebox{50mm}{!} {\includegraphics *{images/deblur/imblur.png}}
\resizebox{50mm}{!} {\includegraphics *{images/deblur/imdeblur.png}}
\caption{Blurred image and the best deblurred output using wiener filter.}
\resizebox{50mm}{!} {\includegraphics *{images/deblur/jia_blind_deconv.png}}
\caption{Blind deconvolution by Jia et. al}
\label{fig:deblur}
\end{center}
\end{figure}

While there was some hint of restoration, the results were not very good,
as the edges blew up. 

To get better results, we used a blind deconvolution algorithm by Abhijit
et al.\cite{abhijith2014}. Instead of a complete blind deconvolution, we used our
kernel as an initial guess and evaluated it against complete blind 
deconvolution result. We found that for highly textured images, the 
output was the same. However, the algorithm converged quickly when we 
used our kernel as an initial estimate. We also run our algorithm for 
very low textured images and found that the results were better than the 
blind case and it took the same time to converge. The results have been
given below.

\begin{figure}[H]
\begin{center}
\resizebox{70mm}{!} {\includegraphics *{images/semiblind/blurred.png}}
\caption{Actual blurred image}
\resizebox{70mm}{!} {\includegraphics *{images/semiblind/blind.png}}
\caption{Deblurring using total blind method}
\resizebox{70mm}{!} {\includegraphics *{images/semiblind/semi_blind.png}}
\caption{Deblurring using our kernel as the initial estimate}
\label{fig:semiblind}
\end{center}
\end{figure}
As is visible from the images above, we find that the deblurring method
with our kernel as the initial estimate gives better results. 
\pagebreak

%----------- Image registration ----------------------------------------
\chapter{IMAGE REGISTRATION}
\label{chap:image_registration}
When taking multiple images, it is very likely that the subsequent 
images will be shifted and rotated. Consider the case of simple shift. 
In the absense of noise, we could use normalized cross correlation
to estimate this shift. However, this method would fail when one of the 
image is blurred. A more sophisticated method would use feature matching
to get a good estimate of a the shift. However, the drawback is that it
is computationally intensive.

To overcome this problem, we can use the inertial sensor data on the 
mobile to get a good estimate of the change in orientation, since the 
inertial sensors give direct information about the motion during the 
photography period. 

Before we go into the case of pure translation, we need to understand 
the limitation of our current setup. We have a 3-axis accelerometer 
giving us the acceleration along ${x, y}$ and $z$ axes, relative to
the mobile frame of reference. Since our setup constrains that we have
only in-plane motion, the acceleration along $z$ axis does not give 
any information. Any generic motion in the plane can be represented by
three entities, ${t_x, t_y}$ and $R_z$. Hence, we have three unknowns
and only two equations from the two axes, which makes it an 
indeterminate system. Hence, we constrain the system by saying that
either the system goes through only translation or only rotation. Hence,
we have the following equations:

\begin{align*}
t_{x_i}=\int{\int{a_x}}dt\\
t_{y_i}=\int{\int{a_y}}dt\\
or\\
{\theta}_i=\tan^{-1}({a_y}/{a_x})\\
\end{align*}

We look at two cases, one of pure translation and another of pure 
rotation.
\section{Pure translation}
\label{image_registration:pure_translation}
As mentioned in \ref{device:device_app:cam}, we can determine the 
trajectory of the device from the accelerometer data. To evaluate our
algorithm, we take a pair of images, one without blur and one with blur
with a 500ms duration between them. The accelerometer data will give a
rough estimate of the final position moved given by:

\begin{align*}
x_{\text{\emph{final}}}=\int_{t=t_{init}}^{t_{final}}
\int_{t=t_{init}}^{t_{final}}{a_xdt}\\
y_{\text{\emph{final}}}=\int_{t=t_{init}}^{t_{final}}
\int_{t=t_{init}}^{t_{final}}{a_ydt}
\end{align*}
The above final position is in mm and needs to be converted 
into pixel dimension. However, we don't have the distance of the scene
from the camera, which makes it an ill-posed problem. To overcome this, 
we iterate through various scales and find out the correct scale using 
RMS error. Hence,
\begin{align*}
(x_{shift}, y_{shift}) = argmin_k\{\sum_x\sum_y(im_1(x,y)
-im_2(x-k*x_{final},y-k*y_{final}))^2\}
\end{align*}
Note that the maximum shift is bound due to the finite duration. Hence,
our search space is drastically reduced from $360^o$ in the blind
case to a small sector(it is not a straight line due to the drift 
associated with the accelerometer readings) with the help of
accelerometer data. 

The following images are example outputs of the algorithm. While the overlap
is not exact, the finer registration can be done with more sophisticated
algorithm. 

\begin{figure}[H]
\begin{center}
\includegraphics[width=300pt]{images/imreg/shift/eg1/imreg.png}
\caption{Example 1: Registered image pairs}
\includegraphics[width=300pt]{images/imreg/shift/eg2/imreg.png}
\caption{Example 2: Registered image pairs}
\end{center}
\end{figure}

\section{Pure rotation}
\label{image_registration:pure_rotation}
Assuming only in-plane rotation, we can relate the measured acceleration
to world acceleration (in the absense of other forces) as
\begin{align*}
a_x=g\cos\theta\\
a_y=g\sin\theta
\end{align*}
Where $\theta$ is the orientation of the device. Hence, the orientation
of the mobile can be estimated as
\begin{align*}
\theta=\tan^{-1}(\frac{a_y}{a_x})
\end{align*}
As in the pure translation case, we take two images, one non-blurred and
another blurred, both separated by 500ms duration. We only rotate the 
device during this duration. Hence, we can use the acceleration values
estimated during this time frame to correct for the rotation. Note that
the process of rotation will also induce a small translation in the 
image. This makes the RMS error method of finding the appropriate
angle difficult. We hence use normalized cross correlation, which is 
independent of the shift. Thus,
\begin{align*}
% Bonkers. This does not make sense. 
\theta=argmax_t\big\{ifft(\frac{fft(im_1).fft(rotate(im_2,\theta_t))^*}
{|fft(im_1).fft(rotate(im_2,\theta_t))|})\}
\end{align*}
Where $fft(.)$ is the 2D discrete fourier transform, $ifft(.)$ is the 2D 
discrete inverse fourier transform and $rotate(im, \theta)$ rotates the
 image by $\theta$ degrees.

The following images serve as an example for the algorithm. In the first
and second example, the first image, which is taken with long exposure
is a highly blurred image. As is evident from the images, the algorithm
performs very good. The third image is a small rotation case and our
algorithm performs well for that case too.

\begin{figure}[H]
\begin{center}
\includegraphics[width=300pt]{images/imreg/rotation/eg1/imreg.png}
\caption{Example 1: Very high blur and rotation}
\includegraphics[width=300pt]{images/imreg/rotation/eg2/imreg.png}
\caption{Example 2: Moderate blur and rotation}
\includegraphics[width=300pt]{images/imreg/rotation/eg3/imreg.png}
\caption{Example 3: Very small blur and rotation}
\end{center}
\end{figure}

\pagebreak
%----------- Depth estimation ------------------------------------------
\chapter{DEPTH ESTIMATION}
\label{chap:depth_estimation}
When taking picture, due to the variable depth of the scene, the amount 
of blur induced due to defocus or motion of the camera is different. 
When estimating depth, we use this fact that one of the parameters of 
the image is sensitive to depth. We particularly look at two types of 
depth estimation, namely depth from motion blur and shape from focus.

\section{Depth from motion blur}
\label{depth_estimation:motion}
Section \ref{basic_theory:image_blur:motion} gives a brief discussion 
about how every point in the image gets blurred by a different blur 
kernel due to the depth profile. If we neglect the parallax error, we 
can see that the only variation in the blur kernel is the scale. Hence,
estimating the depth profile boils down to estimating the scale of the 
blur kernel at different points. Estimating depth from a blurred and 
latent image has been discussed in \cite{paramanand2010unscented}. However,
the method requires estimation of depth at every point, due to the 
absence of any motion information. \cite{levin2007image} uses coded 
aperture for extracting the depth profile from the image. However, the
paper has good discussion about estimating depth from two images, which
is more apt for our discussion.

In this setup, we capture two images, one with very short exposure and
another with long exposure. The short exposure image is captured by
copying the image buffer. While a better choice would be to take capture
an image with very short exposure, it will tend to slow down the process
and induce more delay between the two images. 

Further, along with the two images, the motion of the camera is 
estimated by logging the accelerometer values. Thus, we have the latent
image, the blurred image and the blur kernel. We need to estimate the 
scale of the blur kernel at each point. Prior to estimating the depth,
it is very important that the two images are aligned. We would show that
misalignment could be a major reason for wrong depth profiles. However,
for small shifts, we found that image registration using cross 
correlation gave good results

To estimate the depth at each point, we iterate through various blur
kernel sizes and calculate the difference between the blurred image and
the blurred latent image for each scale of the kernel. If the scale is
correct at a given position, the error at that point will be the least
and that would be assigned as the depth value. Hence, we can formulate
our algorithm as
\begin{align*}
k(x,y) = argmin_d\{|im(x,y)_{blurred}-h(x,y,d)*im_{latent}(x,y)|^2\}
\end{align*}
Where $h(x,y,d)$ is the kernel with scale value of $d$. While this works
for the ideal case, the real depth map is very noisy. Hence, we low pass
filter the error map with a box filter. This algorithm works very well
when there is good texture in the image. Further, it is necessary that
the kernel is very accurate, else the depth map will go wrong. 

To verify our algorithm, we first tried estimating depth map in a 
sythetic case. We blurred an image with a known depth map
and ran our algorithm. The following results were obtained.
\begin{figure}[H]
\begin{center}
\resizebox{50mm}{!} {\includegraphics *{images/depth/eg0/preview_im.png}}
\resizebox{50mm}{!} {\includegraphics *{images/depth/eg0/saved_im.png}}
\caption{Latent image and synthetic lurred image pair.}
\resizebox{50mm}{!} {\includegraphics *{images/depth/eg0/depth.png}}
\resizebox{50mm}{!} {\includegraphics *{images/depth/eg0/imdepth.png}}
\caption {True depth map and estimated depth map}
\label{fig:depth_synthetic}
\end{center}
\end{figure}
The results verified that our method was useful for estimating depth from 
a scene. We then tried to test our algorithm for real cases. In our
 experiment, we found that the kernel can go to a maximum size of
$20\times20$ and hence we scaled the kernel size from $1\times1$ to 
$20\times20$. We used a filter of size $32\times32$ for the error map.
Some of the experimental results have been given below. 
\begin{figure}[H]
\begin{center}
\resizebox{50mm}{!} {\includegraphics *{images/depth/eg1/preview_im.png}}
\resizebox{50mm}{!} {\includegraphics *{images/depth/eg1/saved_im.png}}
\caption{Latent image and Blurred image pair. This is an example of 
close up shot.}
\resizebox{50mm}{!} {\includegraphics *{images/depth/eg1/imdepth.png}}
\caption {Estimated depth map. Objects at the front are brighter}
\label{fig:depth_eg1}
\end{center}
\end{figure}
Observe that the image pair are very well aligned already and the object
is well textured. This made estimation of depth map accurate. However,
the following example is a failure case for estimating depth.
% Put a failure case here. Don't know which is a good idea.
\begin{figure}[H]
\begin{center}
\resizebox{50mm}{!} {\includegraphics *{images/depth/eg2/preview_im.png}}
\resizebox{50mm}{!} {\includegraphics *{images/depth/eg2/saved_im.png}}
\caption{Latent image and Blurred image pair. Note that there is a 
misalignment between the images.}
\resizebox{50mm}{!} {\includegraphics *{images/depth/eg2/imdepth.png}}
\caption {Bad depth map. While the depth calculation at the edges is 
expected to be accurate, the obtained depth map has erroneous edge values.}
\label{fig:depth_eg1}
\end{center}
\end{figure}
To find out the reason for a bad depth map, we perturbed the kernel 
while estimating depth in synthetic case. It was found that if the 
images are misaligned or if the kernel is not accurate, the output depth
map was not accurate. % Need to think if we need to write about SSIM.

\section{Depth from focus}
\label{depth_estimation:focus}
Estimating depth from focus is termed as Structure From Focus (SFF). As
discussed in \ref{basic_theory:image_blur:optical}, varying a parameter
of the camera system changes the quality of the image at different depth.
We are interested in varying the focus of the camera to vary the 
\emph{sharpness} of the image. The sharpness of the image is measured 
using the focus operator. A very popular and easily implementable focus 
operator is the Sum Modified Laplacian (SML). The Modified Laplacian 
operator is defined as
\begin{align*}
\Delta_mI = |I*G_x| + |I*G_y|\\
G_x= [\begin{array}{ccc}-1 & 2 & -1\end{array}],  G_y = G_x^T
\end{align*}
The Sum Modified Laplacian operator does a average of the Modified
Laplacian over the neighbors. Thus,
\begin{align*}
\phi(x,y) = \sum_{i=-N}^{N}\sum_{j=-N}^{N}\Delta_mI(x-i,y-i)
\end{align*}
When a region is in focus, the SML value is the highest. The experiment
consists of taking 100 images with focus varying from minimum to maximum.
For each image, the SML operator is applied and the appropriate depth
for each pixel is when the focus operation peaks.

When changing the focus value, there is a scaling in the images. The 
robust way to correct this would be to register each image with the
first image using Fourier Mellin Transform or feature based image
registration. However, we found that finding the scale of the final 
image with respect to the first image and interpolating the scale for all
other images worked good. Further, this scaling was consistent across all
the experiments and hence finding the necessary scaling was a one time
operation. 

The following image pair is the first and the last image in the 100 images.
\begin{figure}[ht]
\begin{center}
\resizebox{60mm}{!} {\includegraphics *{images/focus/eg1/im3.png}}
\resizebox{60mm}{!} {\includegraphics *{images/focus/eg1/im99.png}}
\caption{First and the last image of the 100 images. Scale hasn't been
corrected.}
\label{fig:focus_first_last}
\end{center}
\end{figure}\\

To evaluate our algorithm, we used a filter size of $32\times32$. Further,
to confirm that our algorithm returns the correct depth map, we created
an all focus image from the set of images. The following are the obtained
results.
\begin{figure}[H]
\begin{center}
\resizebox{100mm}{!} {\includegraphics *{images/focus/eg1/imdepth.png}}
\caption{Estimated depth map. Objects nearer are darker}
\resizebox{100mm}{!} {\includegraphics *{images/focus/eg1/imfocus.png}}
\caption{All focused image. }
\label{fig:depth_focus}
\end{center}
\end{figure}
From the depth map, we see that the objects have been segregated on the
basis of depth. Further, the areas of low texture don't respond very
well, as the SML operator is a high frequency measurement operator. 

\pagebreak
%----------- Image superresolution -------------------------------------
\chapter{IMAGE SUPERRESOLUTION}
\label{chap:image_superresolution}
Image super resolution involves taking multiple low resolution images
and combining them to get a single high resolution image. For example,
a set of four megapixel shots can be combined to get a single sixteen
megapixel shot. This is particularly useful because creating a sensor 
for a single sixteen megapixel camera is very costly. With introduction
of high performance microprocessors on mobiles, the burden of high 
resolution pictures can be shifted from the sensor to the computing side.

Super resolution can be broken down into two problems, Image registration
and Image restoration. The set of images need to be 
registered very accurately initially. Registration can be done in 
multiple ways, as was discussed in \ref{basic_theory:registration}. Note
that we need sub-pixel accuracy and hence a simple cross correlation 
method will not yield correct results. Information from accelerometer
obtained along with the set of images can prove very useful as it gives
the trajectory of motion, thus making accurate registration very easy. 
Due to lack of time, we could not verify the usefulness of the accelerometer
data. 

Once the images are registered, the
super resolved image can be restored. Some of the methods include the
interpolation methods like the bicubic method, spline interpolation or 
even frequency methods. Simple methods like bicubic method would work 
in the case of very high accuracy registration. 

To show the proof of concept, we logged images continuously from the 
mobile phone at an interval of around 20ms. These images were then 
used for super resolving to a scale of 2. For this purpose, we used an
off the shelf code by Sune et al. To compare the results, we rescaled a
single image using bicubic interpolation. One example is presented 
below

\begin{figure}[H]
\begin{center}
\resizebox{120mm}{!} {\includegraphics *{images/super_resolution/imlr3.png}}
\caption{Low resolution image scaled using bicubic interpolation}
\resizebox{120mm}{!} {\includegraphics *{images/super_resolution/imsr3.png}}
\caption{Super resolved image}
\label{fig:super_resolution}
\end{center}
\end{figure}

From the figure, it is clear that the super resolution method gives a 
sharper output when compared to bicubic interpolation method. To quantify
the output, we measured the SSIM between the two images and found it to
be 0.997192, which is high, but says that the super resolved image contains
more information as compared to the bicubic method. The disparity map 
for the two images is shown below. Note that there is high disparity 
at the edges.

\begin{figure}[H]
\begin{center}
\resizebox{120mm}{!} {\includegraphics *{images/super_resolution/diff.png}}
\caption{Disparity map of the two images}
\label{fig:super_resolution}
\end{center}
\end{figure}

However,
the code used was very slow. It took close to 10 minutes for calculating
the optical flow, necessary for registering the images for 5 images of 
size $240\times320$. The image restoration part was very fast. Due to
absence of additional data, the optical flow calculation method was not
fast enough. This part can be replaced by the information from the motion
sensors to speed up the code. 
\pagebreak

%----------- Conclusion ------------------------------------------------
\chapter{CONCLUSION}
\label{chap:conclusion}
We have evaluated some of the important image restoration and registaration
algorithm with data from a mobile. Some of the results are very promising,
while some of them weren't very good. In particular, estimating depth
from focus, image registration and image super resolution have given 
very good results. Depth estimation from blurred and latent image pair
was good to a certain extent but was very sensitive to the accuracy of
the kernel estimate. The results can be made better with the presense
of a magnetometer, which would provide accurate orientation information,
thus making the estimate of acceleration more accurate. Not much work
was done on the image super resolution part, but the off the shelf code
worked. The image deblurring, while proving interesting results like fast
convergence and better results in case of low texture images, has good 
scope for improvement.

While all the codes were run offline for ease of research purpose, we 
saw that the algorithms were very simple and efficient due to availability
of information from motion sensors. Hence, porting these algorithms onto
a mobile should not be a problem and they would work very fast.  

\section{Future work}
Along with the evaluation of major algorithms for registration and 
restoration, the project has provided a good stage for future developments.
All the discussed algorithms can be immediately ported to a mobile 
platform with great ease, which would provide a very powerful 
computational imaging device.

Apart from the usefulness of our algorithms, the mobile application is a
very versatile one and would serve well for future experiments with 
different algorithms. With features like continuous logging of images, 
continous logging of motion sensor data, capturing low exposure noisy 
image and long exposure blurred image and freely changing parameters of 
the camera, the device would prove very useful for working on 
computational imaging on a mobile platform. With powerful processor and
easy to use code, not much needs to be worried about porting existing 
algorithms to make the mobile more powerful.

Apart from the sunny side of the project, there were some parts which 
were not addressed appropriately. Not much effort was put into writing 
a more efficient semi-blind deconvolution algorithm. However, this was
partly not possible due to absence of magnetometer on our device. Further,
the mobile currently does all the job using a 10ms timer, which might
be inefficent. Unloading the work onto a separate thread will not only
make it neat, but it would make it much more efficient. 

We are looking forward to open our code to public so that people who are
interested can contribute. The code will be the property of the lab and 
the author of the report would like to credit all the work to the lab.
\pagebreak


%----------- Bibliography ----------------------------------------------
\begin{singlespacing}
	\bibliographystyle{IEEEtran}
	\nocite{*}
	\bibliography{refs}
\end{singlespacing}

\end{document}