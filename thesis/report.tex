% Use the IITM Dissertion class
\documentclass[BTech]{iitmdiss}
\usepackage{times}
\usepackage{t1enc}

\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage[hypertex]{hyperref} % hyperlinks for references.
\usepackage{amsmath} % easier math formulae, align, subequations \ldots

% Enable code snippets coloring
\usepackage{listings}
\usepackage{xcolor}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

% Python details
\lstdefinestyle{pylisting}{frame=none,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true
  tabsize=2
}

% C# details
\lstdefinestyle{sharpclisting}{frame=none,
  language=[Sharp]C,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true
  tabsize=2
}

\lstset{language=Python, frame=none}
\lstset{language=[Sharp]C, frame=none}

\begin{document}
%----------- Title page ------------------------------------------------
\title{Analyzing motion sensors on mobile platform for image restoration}
\author{Saragadam R V Vishwanath}
\date{MAY 2014}
\department{ELECTRICAL ENGINEERING}
\maketitle
\pagebreak

%----------- Certificate -----------------------------------------------
\certificate

\vspace*{0.5in}

\noindent This is to certify that the thesis titled {\bf Analyzing
 motion sensors on mobile platform for image restoration}, submitted by
  {\bf Saragadam Raja Venkata Vishwanath}, 
  to the Indian Institute of Technology, Madras, for
the award of the degree of {\bf Bachelor of Technology}, is a bona fide
record of the research work done by him under our supervision.  The
contents of this thesis, in full or in parts, have not been submitted
to any other Institute or University for the award of any degree or
diploma.

\vspace*{1.5in}

\begin{singlespacing}
\hspace*{-0.25in}
\parbox{2.5in}{
\noindent {\bf Prof. A. N. Rajagopalan} \\
\noindent Research Guide \\ 
\noindent Professor \\
\noindent Dept. of Electrical Engineering\\
\noindent IIT-Madras, 600 036 \\
} 
\hspace*{1.0in} 
\end{singlespacing}
\vspace*{0.25in}
\noindent Place: Chennai\\
Date: 12th May 2014

\pagebreak

%----------- Acknowledgement -------------------------------------------
\acknowledgements

I would like to express my heartfelt gratitude to my guide, 
Dr. A. N. Rajagopalan for his invaluable support and guidance throughout
my project duration. I have gained immense insight into computational
photography and image processing in this one year, which I believe will 
hold me in good stead in my future endeavors. I am indebted to him for
patiently sitting with me at times when I could not see any path forward
. 

\noindent I also thank all my lab mates for being extremely helpful and for their
patience and guiding me through difficult tasks. I had the privilege of 
working with some of the best minds in the department.

\noindent I would also like to thank my parents and my brother for giving me 
feedback on my project work by evaluating my mobile application.

% Need to write my name on the right side.
\raggedright{Vishwanath}

%----------- Abstract --------------------------------------------------
\abstract
We explore various image restoration and registration techniques using 
data obtained from a mobile device. To evaluate the usability of the 
mobile platform, we evaluate image deconvolution, image registration,
depth estimation and image super resolution algorithms with the obtained
image data. We compare the results to the state of the art algorithms
to get a feel for the superiority of the mobile platform. Further, the
report also gives an indepth discussion about the future of the project
on computational photography on the mobile platform.

%----------- Table of content ------------------------------------------
\begin{singlespace}
\tableofcontents
\thispagestyle{empty}

\listoftables
\addcontentsline{toc}{chapter}{LIST OF TABLES}
\listoffigures
\addcontentsline{toc}{chapter}{LIST OF FIGURES}
\end{singlespace}
\pagebreak

% We might not need abbrevations and notations.
%----------- Abbrevations ----------------------------------------------
\abbreviations

\noindent 
\begin{tabbing}
xxxxxxxxxxx \= xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \kill
\textbf{IITM}   \> Indian Institute of Technology, Madras \\
\textbf{WP8}    \> Windows Phone 8 \\
\end{tabbing}

\pagebreak

%----------- Notations -------------------------------------------------
\chapter*{\centerline{NOTATION}}
\addcontentsline{toc}{chapter}{NOTATION}

\begin{singlespace}
\begin{tabbing}
xxxxxxxxxxx \= xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \kill
\textbf{$f$}    \> Latent image \\
\textbf{$g$}    \> Observer/blurred image \\
\textbf{$k$}    \> Blur kernel \\
\textbf{$im(x,y)$}  \> Image intensity at location (x, y) \\
\end{tabbing}
\end{singlespace}

\pagebreak

% Start numbering the pages from now.
\pagenumbering{arabic}

%----------- Introduction ----------------------------------------------
\chapter{INTRODUCTION}
\label{chap:intro}
There has been a recent shift in computation from traditional computers
to ubiquitous mobiles. With the ever increasing power of the mobiles, 
executing many of the image processing algorithms on mobile has been 
possible. Further, the many peripherals available for the mobile, like
the inertial sensors, controllable focus, TCP, bluetooth etc., 
computational photography is greatly advanced. 

Using the mobile platform for computational photography has the unique
advantages of capturing motion during exposure time, which can be used
for estimating the blur kernel or estimating the direction of motion
for reducing the search space of image registration. Further, 
controllable focus and the ability to capture fast multiple images means
that it is easy to implement multichannel super resolution or deblurring
algorithms. 

To understand the potential of a mobile for computational photography, 
we choose to evaluate various algorithms on the mobile and evaluate the 
performance. For this purpose, we have chosen the Nokia Lumia 520 mobile
which features Windows Phone 8 operating system. 

The project report consists of the following part. The chapter on 
\textbf{THEORY} presents a brief background of the various algorithms
and the mathematics involved in the project. \textbf{DEVICE} gives an
in-depth account of the mobile platform, the application written for the
mobile platform and the host side software. \textbf{DEBLURRING} 
explains the implementation of the deblurring algorithm using the data
obtained from the mobile. \textbf{DEPTH ESTIMATION} has two parts, 
\emph{Depth estimation using motion blur} and \emph{Depth estimation 
using focus measure}. \textbf{IMAGE REGISTRATION} discusses about how
inertial sensor data can be used to reduce the search space of image 
registration process. Finally, \textbf{SUPER RESOLUTION} looks at how 
multiple frames can be used to super resolve an image.

The end of the project report has in-depth discussion regarding future
usage of the mobile application as a general prototyping platform and a
brief discussion of the \emph{github} version control system.

%----------- Basic Theory ----------------------------------------------
\chapter{BASIC THEORY}
\label{chap:basic_theory}
In this chapter, we look briefly at the back ground which is necessary 
to understand the forthecoming chapters. 

\section{Image blur}
\label{basic_theory:image_blur}
Image blur is equivalent to low pass filtering in 1D signals. While 
image blur is desired in some applications like animation, it is 
undesirable when taking still photos. Two types of blurs are of interest
in this project, namely motion blur and defocus blur

\subsection{Defocus blur}
\label{basic_theory:image_blur:optical}
Defocus blur is caused when the image plane does not match with the 
optical plane of the object. This arises only in real aperture cases
when the focal length of the lens is finite and objects are at varied
distances. 

The blur that arises due to defocus can be modeled as a 2D gaussian
filtering operation. Hence, the point spread function can be modeled as

\begin{align*}
h(x,y) = \frac{1}{2\pi\sigma^2}e^\frac{-(x^2+y^2)}{2\sigma^2}
\end{align*}
Where $\sigma$ depends on the depth of the object from the lens. For a 
given lens to sensor distance, $\sigma$ increases with increasing depth.

The defocus blur can be used in the shape from focus method, which 
varies one particular parameter of the system like the lens position or
the object position to bring the object in focus. This variation gives 
an estimate of the depth. Further discussion about shape from focus will
be carried out in the chapter on Depth Estimation.

\subsection{Motion blur}
Motion blur occurs due to relative motion between the scene and the 
camera during the exposure time. This could mean the object in the scene
is moving or there is a camera shake. This report deals with the problem
of camera shake.

The process of camera shake can be considered as an averaging of 
multiple sharp images shifted due to the camera motion. Hence for a 
planar scene, 
\begin{align*}
\hat{im}(x,y) = \Sigma_tim(x-x_t,y-y_t)\\
\implies\hat{im}(x,y) = \Sigma_t\delta(x_t, y_t)*im(x,y)
\end{align*}
where $\delta(x_t, y_t)$ is an impulse at $(x_t, y_t)$ and $\hat{im}$ is
the observed image.

This can be rewritten as
\begin{align*}
\hat{im}(x,y) = (\Sigma_t\delta(x_t, y_t))*im(x,y)\\
\implies \hat{im}(x,y) = h(x,y)*im(x,y)
\end{align*}

$h(x,y)$ is called the point spread function. If the image is not a 
planar scene, then every point in the scene moves by a different amount
due to the camera shake. If we assume that there is no parallax error,
we have,
% Do we need to write about why they move by different lengths?
\begin{align*}
\hat{im}(x,y) = \Sigma_t\delta(x_t.k(x,y), y_t.k(x,y))*im(x,y)
\end{align*}
where $k(x,y)$ is the depth of the image at $(x,y)$. This space variant
blurring operation can be used to calculate depth information from the 
images, as objects at different depths will be blurred to a different
extent. Calculating $k(x,y)$ gives the depth map of the image

\section{Image deconvolution}
\label{basic_theory:deconv}


%----------- Device ----------------------------------------------------
\chapter{DEVICE}
\label{chap:device}
While choosing the mobile platform, we wanted to look at the following
features
\begin{itemize}
\item The device should be low cost. We are looking at a mobile which
would not cost above Rs. 15,000. This gave us the option to choose 
Android mobiles or the low end Windows Phone Mobiles.
\item The device should have a good Software Development Kit. Android
and Windows Phone both have a very good SDK.
\item The device should have accelerometer, gyroscope and manual focus
ability. Almost all the mobiles which come today have these features 
and hence was not very critical.
\item The device should have easy image handling capabilities. Here, 
Windows Phone was a clear winner due to the powerful Nokia Imaging SDK.
\item The device should have TCP/Bluetooth capability. Again, every 
smart phone available today has these features. 
\end{itemize}
 
With the above points in mind and considering the low cost, we chose to
go for Nokia Lumia 520 (henceforth called 520) which features a 5 MP
camera, 3-axis accelerometer, WiFi and Blutooth support. However, the 
tradeoff was that we could not get a gyroscope on the mobile. This 
would be a serious hinderance for handling rotation of the mobile along
with translation. However, since we were only evaluating the 
possibilities of computational photography on the mobile platform, we
chose to ignore it and see what can be done with the accelerometer 
alone. 

From the ease of usage perspective, the Windows Phone 8 SDK has a very
simple interface with coding in C\# and GUI design using XAML script. 
Visual Studio makes it very easy to create applications (apps) fast.

% Device side application
\section{Device side application}
\label{device:device_app}
The app is mainly used for logging information to the computer so that
algorithms can be tested offline. The major advantage of this method is
that it gives the power of the computer along with the flexibility to 
experiment with various methods. 
 
To make the application versatile and useful for future development, we
added multiple features to capture images. A screenshot of the 
application is given below.
\begin{figure}[htpb]
    \begin{center}
        \resizebox{100mm}{!} {\includegraphics *{images/app_screenshot.png}}
        \caption {A screenshot of the application}
        \label{fig:app_screenshot}
    \end{center}
\end{figure}
The top right, \textbf{Information block} serves as a debugging block 
during the image capturing process. For example, when taking images with
long exposure, it indicates the number of samples of accelerometer data
obtained. The \textbf{Debug section} box indicates the focus position and other
debug information. The focus position can be controlled by the yellow
slide bar which is situated at the bottom of the application. 

The \textbf{Accelerometer} box indicates the
current value of the accelerometer. The button titled \textbf{Click} is 
used for capturing a single long exposure image. \textbf{Connect} button
is used for connecting to the computer using the TCP protocol. The Host
and the port can be selected in the fields at the right bottom of the 
application screen. Apart from these features, the following features 
add versatility to the application

\begin{itemize}
    \item \textbf{Log images} enables continuous logging of the image 
    frames to the computer when TCP stream is open. With the default
    setting, the duration between two frames is close to 20ms.
    \item \textbf{Enable delay} adds a 500 ms delay between the image
    retrieved from the image buffer and the long exposure shot.
    \item \textbf{Enable preview} captures an image from the image buffer
    (which can act as a latent image) prior to a long shot image. 
    \item \textbf{Start sensor log} enables continous logging of the 
    accelerometer data to the computer when the TCP stream is open. 
    \item \textbf{Focus sweep} records 100 images with varied focus 
    distance and sends them to the computer.
\end{itemize}

The application has a 10ms timer, which takes care of all the logging 
and image capture functions. While it is more apt to use a separate 
thread, we found that using a timer was an easy method. 

We discuss some of the methods implemented for logging the data and 
sending it over TCP to understand the intricacies involved.

% Logging accelerometer data during exposure
\subsection{Logging accelerometer data during exposure}
\label{device:device_app:cam}
When the \textbf{Click} button is toggled, a camera capture sequence
is started, which is programmed to take a picture with an exposure time
of 200ms (adjustable). However, by default, this suspends all other 
thread activities and is only released once the capture is complete. 
This poses the problem that the accelerometer data cannot be logged 
during this time, which is very important for estimating the blur kernel
. To overcome this, we use the async mode of camera capture(add ref to
async camera capture website). In this mode, the camera is initialized
for capture and it returns to the thread. Meanwhile, a busy flag is set
and accelerometer data is recorded. When this is done, the busy flag is
unset, which stops the data logging. The code snippet is given below

\begin{singlespacing}
\begin{lstlisting}[style=sharpclisting]
public async void capture(bool get_preview, bool register)
{
    // Get image preview
    if (get_preview == true)
    {
        _camera.GetPreviewBufferArgb(preview_image);
    }
    // Take a picture. Flag busy meanwhile.
    cam_busy = true;
    // If register mode is enabled, sleep for 500ms.
    if (register == true)
    {
        await System.Threading.Tasks.Task.Delay(500);
    }
    await _camsequence.StartCaptureAsync();
    cam_busy = false; // Done capture. Unset busy.
    transmit = true;
    imstream.Seek(0, SeekOrigin.Begin);
}
\end{lstlisting}
\end{singlespacing}

If an exposure time of 200ms is used, 20 accelerometer values are 
expected with a 10ms timer. However, close to 70 readings are obtained.
This is due to the transfer time of the image data also included in the
capture time. To overcome this problem, a fast moving image of a single
point light source is taken. Frames of 20 out of 70 samples of the
accelerometer data is used for estimating the best start and end times.
We use these start and end times for all the future kernel estimates.
%% We could add an image for reference here.

% Sending data over TCP
\subsection{Sending data over TCP}
\label{device:device_app:tcp}
The TCP communication enables a communication stream between the mobile
device and the computer. With the stream open, it can be treated as a 
continuous information channel, with no prior information about the size
of the data being sent. To works with this \emph{asynchronous} type of
data transmission, we encapsulate the data in keywords which is known to
the computer. This would boil down the problem to searching for the 
starting and ending keyword for each data type received. For example, if
we are sending a new image, we would use the keywords \textbf{STIM} and
\textbf{EDIM} to encapsulate the image data. An example: 

\begin{singlespacing}
\begin{lstlisting}[style=sharpclisting]
    // app_comsocket is the object representing the TCP connection.
    // app_comsocket.Send is used to send data over the connection.
    app_comsocket.Send("STIM\n");  // Start token
    app_comsocket.Send(imdata);     // Image data
    app_comsocket.Send("EDIM\n");  // End token
\end{lstlisting}
\end{singlespacing}

% Host side application
\section{Host side application}
\label{device:host}
Most of the host side application is written in python. The reason for 
choosing python is the vast support available for scientific computing,
communication protocols, image processing support and above all, python
being free software. However, some of the deblurring codes and 
super resolution are in Matlab.

The communication from mobile to computer is done using the inbuilt
\emph{socket} module in python. As mentioned in 
\ref{device:device_app:tcp}, the data from the mobile is sent
encapsulated in keywords. These keywords are used to parse the incoming
data and split it accordingly. 

\pagebreak

%----------- Deblurring ------------------------------------------------
\chapter{DEBLURRING}
\label{chap:deblurring}
\pagebreak

%----------- Depth estimation ------------------------------------------
\chapter{DEPTH ESTIMATION}
\label{chap:depth_estimation}

%----------- Image registration ----------------------------------------
\chapter{IMAGE REGISTRATION}
\label{chap:image_registration}
When taking multiple images, it is very likely that the subsequent 
images will be shifted and rotated. Consider the case of simple shift. 
In the absense of noise, we could use the normalized cross correlation
to estimate this shift. However, this method would fail when one of the 
image is blurred. A more sophisticated method would use feature matching
to get a good estimate of a the shift. However, the drawback is that it
is computationally intensive.

To overcome this problem, we can use the inertial sensor data on the 
mobile to get a good estimate of the change in orientation, since the 
inertial sensors give direct information about the motion during the 
photography period. 

Before we go into the case of pure translation, we need to understand 
the limitation of our current setup. We have a 3-axis accelerometer 
giving us the acceleration along ${x, y}$ and $z$ axes, relative to
the mobile frame of reference. Since our setup constrains that we have
only in-plane motion, the acceleration along $z$ axis does not give 
any information. Any generic motion in the plane can be represented by
three entities, ${t_x, t_y}$ and $R_z$. Hence, we have three unknowns
and only two equations from the two axes, which makes it an 
indeterminate system. Hence, we constrain the system by saying that
either the system goes through only translation or only rotation. Hence,
we have the following equations:

\begin{align*}
t_{x_i}=\int{\int{a_x}}\\
t_{y_i}=\int{\int{a_y}}\\
or\\
{\theta}_i=tan^{-1}({a_y}/{a_x})\\
\end{align*}

We look at two cases, one of pure translation and another of pure 
rotation.
\section{Pure translation}
\label{image_registration:pure_translation}
As mentioned in \ref{device:device_app:cam}, we can determine the 
trajectory of the device from the accelerometer data. To evaluate our
algorithm, we take a pair of images, one without blur and one with blur
with a 500ms duration between them. The accelerometer data will give a
rough estimate of the final position moved given by:

\begin{align*}
x_{\text{\emph{final}}}=\int_{t=t_{init}}^{t_{final}}
\int_{t=t_{init}}^{t_{final}}{a_xdt}\\
y_{\text{\emph{final}}}=\int_{t=t_{init}}^{t_{final}}
\int_{t=t_{init}}^{t_{final}}{a_ydt}
\end{align*}
The above final position is in mm and needs to be converted 
into pixel dimension. However, we don't have the distance of the scene
from the camera, which makes it an ill-posed problem. To overcome this, 
we iterate through various scales and find out the correct scale using 
RMS error. Hence,
\begin{align*}
(x_{shift}, y_{shift}) = argmax_k\{\Sigma_x\Sigma_y(im_1(x,y)
-im_2(x-k*x_{final},y-k*y_{final}))^2\}
\end{align*}
Note that the maximum shift is bound due to the finite duration. Hence,
our search space is drastically reduced from $360^o$ in the blind
case to a small sector(it is not a straight line due to the drift 
associated with the accelerometer readings) with the help of
accelerometer data. 

The following images are example outputs of the algorithm. While the overlap
is not exact, the finer registration can be done with more sophisticated
algorithm. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=300pt]{images/imreg/shift/eg1/imreg.png}
\caption{Example 1: Registered image pairs}
\includegraphics[width=300pt]{images/imreg/shift/eg2/imreg.png}
\caption{Example 2: Registered image pairs}
\end{center}
\end{figure}

\section{Pure rotation}
\label{image_registration:pure_rotation}
Assuming only in-plane rotation, we can relate the measured acceleration
to world acceleration (in the absense of other forces) as
\begin{align*}
a_x=gcos\theta\\
a_y=gsin\theta
\end{align*}
Where $\theta$ is the orientation of the device. Hence, the orientation
of the mobile can be estimated as
\begin{align*}
\theta=tan^{-1}(\frac{a_y}{a_x})
\end{align*}
As in the pure translation case, we take two images, one non-blurred and
another blurred, both separated by 500ms duration. We only rotate the 
device during this duration. Hence, we can use the acceleration values
estimated during this time frame to correct for the rotation. Note that
the process of rotation will also induce a small translation in the 
image. This makes the RMS error method of finding the appropriate
angle difficult. We hence use normalized cross correlation, which is 
independent of the shift. Thus,
\begin{align*}
% Bonkers. This does not make sense. 
\theta=argmax_t\big\{ifft(\frac{fft(im_1).fft(rotate(im_2,\theta_t))^*}
{|fft(im_1).fft(rotate(im_2,\theta_t))|})\}
\end{align*}
Where $fft(.)$ is the 2D discrete fourier transform, $ifft(.)$ is the 2D 
discrete inverse fourier transform and $rotate(im, \theta)$ rotates the image
by $\theta$ degrees.

The following images serve as an example for the algorithm. In the first and
second example, the first image, which is taken with long exposure is a highly
blurred image. As is evident from the images, the algorithm performs very good. 
The third image is a small rotation case and our algorithm performs well for
that case too.

\begin{figure}[h]
\begin{center}
\includegraphics[width=300pt]{images/imreg/rotation/eg1/imreg.png}
\caption{Example 1: Very high blur and rotation}
\includegraphics[width=300pt]{images/imreg/rotation/eg2/imreg.png}
\caption{Example 2: Moderate blur and rotation}
\includegraphics[width=300pt]{images/imreg/rotation/eg3/imreg.png}
\caption{Example 3: Very small blur and rotation}
\end{center}
\end{figure}

\pagebreak
%----------- Image superresolution ---------------------------------------------
\chapter{IMAGE SUPERRESOLUTION}
\label{chap:image_superresolution}
\pagebreak

\end{document}

